# -*- coding: utf-8 -*-
"""NN training (1)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fF4UaMQAfL7HSk_XoMQ6HMmtPTz6ugnh
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!pip -q install adjustText

# —— installation of special fonts ——
def use_cjk_font():
    import matplotlib, matplotlib.pyplot as plt
    from matplotlib import font_manager as fm, rcParams
    import glob, os

    # 1) Find any Noto CJK (or Source Han Sans) font in the system.
    candidates = []
    candidates += glob.glob("/usr/share/fonts/**/NotoSansCJK*.*", recursive=True)
    candidates += glob.glob("/usr/share/fonts/**/NotoSerifCJK*.*", recursive=True)
    candidates += glob.glob("/usr/share/fonts/**/Noto*Sans*CJK*.*", recursive=True)
    # Can also put own TTF/OTF path:
    # candidates += ["/content/drive/MyDrive/fonts/SimHei.ttf"]

    if not candidates:
        # If you don't have it, install Colab/Ubuntu
        import subprocess, sys
        subprocess.run(["apt","-y","install","fonts-noto-cjk"], check=False, stdout=subprocess.DEVNULL)
        candidates += glob.glob("/usr/share/fonts/**/NotoSansCJK*.*", recursive=True)

    if not candidates:
        raise RuntimeError("找不到任何 CJK 字体，请手动提供 .ttf/.otf 路径")

    path = sorted(candidates)[0]  # Choose one
    fm.fontManager.addfont(path)
    name = fm.FontProperties(fname=path).get_name()

    # 2) Set as default font
    rcParams["font.family"] = name
    rcParams["font.sans-serif"] = [name]
    rcParams["axes.unicode_minus"] = False

    # 3) Clear cache (sometimes old cache can lock up DejaVu)
    import shutil
    cachedir = matplotlib.get_cachedir()
    shutil.rmtree(cachedir, ignore_errors=True)
    fm._load_fontmanager(try_read_cache=False)

    print("CJK font set to:", name, "\nfile:", path)

use_cjk_font()

import matplotlib.pyplot as plt
from matplotlib import font_manager as fm
CJK = fm.FontProperties(family=plt.rcParams["font.family"])

# ==== 0) Configuration and Dependencies (Entire process runs in memory, not writes in disk) ====
import re, os, glob, math, random
from pathlib import Path
import numpy as np
import pandas as pd

# Uniform random seed
SEED = 42
random.seed(SEED); np.random.seed(SEED)

CFG = dict(
    # —— Data path ——
    SILVER_DIR = "/data/NN training/ZHE_silver",  # Directory containing 8 layered .txt files
    GUIDE_MC   = "/data/collection guideline - MC.txt",
    GUIDE_EMOC = "/data/collection guideline - EMoC.txt",

    # —— Window/Model Switch (will be used later during training; placeholder here) ——
    L_CTX = 40, R_CTX = 40,                 # left and right windows
    USE_L1_ONLY = False,                    # Set to False for now; will perform an L1 vs. full-layer comparison later.
    USE_COND_SUB = True,                    # Enable cond_sub small embedding at L1
    SAVE_WEIGHTS = True,                    # Save weights during training phase

    USE_LAYER_FEAT=False,      # M0: False,  M1/M2: True
    USE_COND_SUB_L1=False,

)

# ==== 1.1) Read in 8 layered txt files (merge in memory) ====

def _read_txt_auto(path: Path) -> pd.DataFrame:
    """Automatically guesses delimiters to read TXT files; removes leading and trailing whitespace; preserves the original filename."""
    df = pd.read_csv(path, sep=None, engine="python", dtype=str, encoding="utf-8")
    df.columns = [c.strip() for c in df.columns]
    for c in df.columns:
        if df[c].dtype == object:
            df[c] = df[c].str.strip()
    df["__source_file"] = path.name
    return df

def _parse_epoch_layer_from_name(name: str):
    """Extract epoch (MC/EMoC) and layer (1..4) from filenames."""
    m = re.search(r"(MC|EMoC)_zhe_layer(\d+)", name, flags=re.I)
    if not m:
        return None, None
    epoch = "MC" if m.group(1).upper()=="MC" else "EMoC"
    layer = int(m.group(2))
    return epoch, layer

def load_layered_txts_inmem(dirpath: str) -> pd.DataFrame:
    root = Path(dirpath)
    files = sorted([p for p in root.glob("*layer*.txt*") if p.is_file()])
    if not files:
        listed = [p.name for p in root.glob("*")]
        raise FileNotFoundError(
            f"在 {dirpath} 没找到 *layer*.txt*。\n"
            f"目录下现有文件：\n- " + "\n- ".join(listed[:50])
        )

    parts = []
    for p in files:
        df = _read_txt_auto(p)
        epoch, layer = _parse_epoch_layer_from_name(p.name)
        df["epoch"] = epoch
        df["layer"] = layer
        parts.append(df)
    return pd.concat(parts, ignore_index=True)
    df_all = pd.concat(parts, ignore_index=True)
    return df_all

df_layered = load_layered_txts_inmem(CFG["SILVER_DIR"])
print("layered 合并：", df_layered.shape, " 例列：", df_layered.columns.tolist()[:10])
print(df_layered.groupby(["epoch","layer"]).size().unstack(fill_value=0))

# ==== 1.2) Read in the MC/EMoC guide (pipeline table + bold header compatibility) and build the mapping ====

def _normalize_book(raw: str) -> str:
    """Standardize book titles: remove the part before the '/'; remove spaces."""
    if not isinstance(raw, str): return ""
    s = raw.strip()
    s = s.split("/")[0].strip()
    return s

def _normalize_genre(raw: str) -> str:
    """Genre standardization: Remove parentheses or '/' before the text; remove whitespace."""
    if not isinstance(raw, str): return ""
    s = raw.strip()
    # First cut by Chinese/English brackets, then cut by slashes, and take the leftmost one.
    s = s.split("（")[0].split("(")[0].split("/")[0].strip()
    return s

def _clean_header(names):
    """Remove bold text (*), vertical lines on both sides, and spaces; unify common aliases to Chinese keywords."""
    cleaned = []
    for c in names:
        c2 = str(c)
        c2 = c2.replace("*", "")
        c2 = c2.strip().strip("|").strip()
        cleaned.append(c2)
    return cleaned

def _drop_sep_rows(df: pd.DataFrame) -> pd.DataFrame:
    """Remove markdown separator lines (such as '---')"""
    mask = pd.Series([True]*len(df))
    for col in df.columns:
        s = df[col].astype(str).str.strip()
        mask &= ~s.str.fullmatch(r"-{3,}")  # Lines consisting entirely of ---
    return df[mask].copy()

def _read_guideline_pipe_txt(path: str) -> pd.DataFrame:
    """
    Read the guideline txt (which may be a Markdown pipeline table).
    1) First try `sep=None` (automatic guessing), if that fails, use `sep="|"`;
    2) Clean the table header (remove * and |), discard empty columns and separator lines;
    3) Keep only the columns you are interested in and rename them.
    """
    p = Path(path)
    # — Try to guess the separator automatically —
    try:
        df = pd.read_csv(p, sep=None, engine="python", encoding="utf-8-sig", dtype=str)
        df.columns = _clean_header(df.columns)
    except Exception:
        # — Back to pipe partition —
        df = pd.read_csv(p, sep="|", engine="python", encoding="utf-8-sig", dtype=str)
        df.columns = _clean_header(df.columns)
        # Remove unnamed columns from the parsed output (commonly empty columns at the beginning and end)
        df = df.loc[:, [c for c in df.columns if c and not c.isspace()]]

    # Discard all empty columns
    df = df.dropna(axis=1, how="all")
    # Remove separator lines
    df = _drop_sep_rows(df)

    # Column name matching: compatible with "title/century/category/dynasty"
    # Construct candidate mappings first
    colmap = {c: c for c in df.columns}
    def _find(colnames):
        for k in colnames:
            if k in colmap:
                return k
        return None

    name_key  = _find(["书名","書名","title","Title"])
    cent_key  = _find(["世纪","世紀","century","Century"])
    genre_key = _find(["分类","分類","genre","Genre"])
    ep_key    = _find(["朝代","朝代/时期","epoch","Epoch"])  # Optional

    if not (name_key and cent_key and genre_key):
        raise ValueError(f"指南缺少必要列（书名/世纪/分类）: {path}\n现有列: {list(df.columns)}")

    # Keep only the columns you need and rename them
    keep = [name_key, cent_key, genre_key] + ([ep_key] if ep_key else [])
    df = df[keep].copy().rename(columns={
        name_key: "book_raw",
        cent_key: "century_raw",
        genre_key:"genre_raw",
        **({ep_key:"epoch_raw"} if ep_key else {})
    })

    # Normalization
    df["book_norm"]  = df["book_raw"].apply(_normalize_book)
    df["genre_norm"] = df["genre_raw"].apply(_normalize_genre)
    df["century"]    = df["century_raw"].astype(str).str.strip()
    if "epoch_raw" in df.columns:
        df["epoch_norm"] = df["epoch_raw"].astype(str).str.strip()

    # Remove empty book title lines
    df = df[df["book_norm"]!=""].copy()
    # Deduplication: Only the first entry with the same name will be kept.
    df = df.drop_duplicates(subset=["book_norm"], keep="first")

    return df

def load_guidelines_inmem(path_mc: str, path_emoc: str):
    """返回三张字典：book2century / book2genre / book2epoch（MC/EMoC 来自所属文件）"""
    g_mc   = _read_guideline_pipe_txt(path_mc)
    g_emoc = _read_guideline_pipe_txt(path_emoc)

    # Mark the epoch to which the two tables belong (if epoch_raw is not explicitly provided in the file)
    if "epoch_norm" not in g_mc.columns:   g_mc["epoch_norm"]   = "MC"
    if "epoch_norm" not in g_emoc.columns: g_emoc["epoch_norm"] = "EMoC"

    # Assemble mapping
    def _to_maps(df):
        b2c = dict(zip(df["book_norm"], df["century"]))
        b2g = dict(zip(df["book_norm"], df["genre_norm"]))
        b2e = dict(zip(df["book_norm"], df["epoch_norm"]))
        return b2c, b2g, b2e

    b2c_mc, b2g_mc, b2e_mc   = _to_maps(g_mc)
    b2c_em, b2g_em, b2e_emoc = _to_maps(g_emoc)

    # Merge (if there is a conflict, the latter will overwrite the former; it can also be changed to an alert)
    book2century = {**b2c_mc, **b2c_em}
    book2genre   = {**b2g_mc, **b2g_em}
    book2epoch   = {**b2e_mc, **b2e_emoc}

    return book2century, book2genre, book2epoch

# —— Example of usage (path from the CFG) ——
book2century, book2genre, book2epoch = load_guidelines_inmem(
    CFG["GUIDE_MC"], CFG["GUIDE_EMOC"]
)
print("映射规模：book→century:", len(book2century), " book→genre:", len(book2genre))

# ==== 2）ZHE silver labels (no cond_sub) ====

# --------- a small tool ----------
def _normalize_book(raw: str) -> str:
    """Remove everything before the '/' (ignore the backslash), and remove leading and trailing whitespace."""
    if not isinstance(raw, str): return ""
    return raw.strip().split("/")[0].strip()

# ZHE's set of states
ZHE_STATES = ["Zh-LexV", "Zh-P", "Zh-Dur?", "Zh-Dur"]

def map_zhe_state(row) -> str:
    import re
    lab  = (row.get("label","") or "").strip()
    cond = (row.get("condition","") or "").strip()
    ctx  = (row.get("context","") or "")
    zhe  = (row.get("zhe","") or "")
    nxt  = (row.get("next","") or "")
    full = f"{ctx} {zhe} {nxt}"

    # --- 1) Priority caliber with labels ---
    if lab in {"T1","U1"}:
        # Strong evidence of the TA marker
        if "著(Di/T)" in cond or re.search(r"著\((Di|T)\)", cond):
            return "Zh-Dur"
        # Loosened 'weak proof'
        if ("汉字+著+汉字" in cond) or ("弱证" in cond) or re.search(r"(?<!所)[\u4e00-\u9fff]著[\u4e00-\u9fff]", full):
            return "Zh-Dur?"
        # Explicit verbality
        if re.search(r"著\(V", cond):
            return "Zh-LexV"
        # backup
        return "Zh-LexV"

    if lab in {"T2","U2"}:
        return "Zh-P"

    if lab in {"T3","U3"}:
        return "Zh-LexV"

    if lab in {"T4","U4"}:
        return "Zh-LexV"

    # --- 2) Unlabeled conservative retreat ---
    # Location/Directional Indicators
    if re.search(r"著\(V", zhe) and re.search(r"[上下内中前後东西南北於于在間]", full):
        return "Zh-P"
    # Clearly indicate physical appearance
    if re.search(r"著\((Di|T)\)", full):
        return "Zh-Dur"
    # "所+著" → verb
    if re.search(r"所[\u4e00-\u9fff]*著", f"{ctx}{zhe}"):
        return "Zh-LexV"
    # additional safety net: reversal of weak evidence
    if re.search(r"(?<!所)[\u4e00-\u9fff]著[\u4e00-\u9fff](?=[^\u4e00-\u9fff]|$)", full):
        return "Zh-Dur?"
    # default
    return "Zh-LexV"


def make_silver_zhe(df_layered: pd.DataFrame,
                    normalize_book_fn=_normalize_book) -> pd.DataFrame:
    """
    Input: A four-layer merged long table of ZHE (including epoch/layer/file/context/zhe/next/condition/[label])
    Output: A silver label table (label_silver), without using cond_sub.
    """
    df = df_layered.copy()

    # Standardize book titles for easier alignment with guidelines/metadata later
    if "file" in df.columns:
        df["book_id"] = df["file"].apply(normalize_book_fn)
    else:
        # Some scanners use __source_file
        df["book_id"] = df.get("__source_file", "").astype(str).apply(normalize_book_fn)

    # Uniform context field name
    df["left_ctx"]  = df.get("context","").astype(str)
    df["right_ctx"] = df.get("next","").astype(str)

    # Generate Silver Label
    df["label_silver"] = df.apply(map_zhe_state, axis=1)
    df["label_silver"] = pd.Categorical(df["label_silver"],
                                        categories=ZHE_STATES, ordered=True)

    # Export key columns (fill in blanks for missing columns)
    keep = ["epoch","layer","book_id","left_ctx","right_ctx",
            "zhe","condition","label","label_silver"]
    for k in keep:
        if k not in df.columns:
            df[k] = np.nan
    out = df[keep].copy()

    # Concise Distribution Report
    print("[ZHE silver] label_silver 分布：")
    print(out["label_silver"].value_counts(dropna=False))
    if "epoch" in out.columns:
        print("\n[ZHE silver | 按 epoch]")
        print(out.groupby("epoch")["label_silver"].value_counts(normalize=True).rename("share").round(3))

    return out

# ==== Uniformity and fallback checking ====
try:
    df_layered_zhe = df_layered.copy()
except NameError:
    try:
        df_layered_zhe = df_all.copy()
    except NameError:
        # Backup: Read four layers from BASE (point BASE to your ZHE data directory)
        import pandas as pd
        from pathlib import Path
        BASE = Path("/data/layered data")
        def _load_epoch(e):
            fs = []
            for L in ["layer1","layer2","layer3","layer4"]:
                f = BASE / f"{e}_zhe_{L}.txt"
                df = pd.read_csv(f, sep="\t", dtype=str).fillna("")
                df["layer"] = L
                fs.append(df)
            out = pd.concat(fs, ignore_index=True)
            out["epoch"] = e
            return out
        df_layered_zhe = pd.concat([_load_epoch("MC"), _load_epoch("EMoC")], ignore_index=True)

# --- Generate ZHE Silver Label ---
df_silver_zhe = make_silver_zhe(df_layered_zhe)

# 1) Overall distribution of silver labels
print(df_silver_zhe["label_silver"].value_counts(dropna=False))

# 2) Only look at the "Source Labels/Rules and Conditions → Final Silver Labels" Top 12 in layer 1
# (ZHE does not have cond_sub, so label/condition is used for joint observation)

print(
    df_silver_zhe[df_silver_zhe["layer"].astype(str).eq("layer1")]
      [["label", "condition", "label_silver"]]
      .value_counts()
      .head(12)
)

# 3) Optional: View the percentage by epoch (to easily verify the change from MC to EMoC)
print(
    df_silver_zhe.groupby("epoch")["label_silver"]
      .value_counts(normalize=True)
      .rename("share").round(3)
)

from pickle import FALSE
# ==== 3) ZHE: Splitting & DataLoader (Character-level; M0/M1, M2 = Invalid placeholders) ====
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import GroupShuffleSplit

# —— Labels（ZHE's set of states）——
ZHE_LABELS_FULL = ["Zh-LexV","Zh-P","Zh-Dur?","Zh-Dur"]

# ========== Segmentation: Grouped by book title to prevent leakage ==========
def split_by_book_zhe(df: pd.DataFrame, test_size=0.15, dev_size=0.15, seed=42):
    df = df.copy()
    df = df[df["label_silver"].notna()].copy()
    groups = df["book_id"].astype(str).values
    idx = np.arange(len(df))

    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)
    train_dev_idx, test_idx = list(gss.split(idx, groups=groups))[0]

    groups_td = groups[train_dev_idx]
    gss2 = GroupShuffleSplit(n_splits=1, test_size=dev_size/(1.0-test_size), random_state=seed)
    train_idx, dev_idx = list(gss2.split(train_dev_idx, groups=groups_td))[0]
    train_idx = train_dev_idx[train_idx]
    dev_idx   = train_dev_idx[dev_idx]

    print(f"[ZHE split] train/dev/test = {len(train_idx)}/{len(dev_idx)}/{len(test_idx)} (group=book)")
    return df.reset_index(drop=True), train_idx, dev_idx, test_idx

# ========== Vocabulary & Encoding (Center character set to ZHE) ==========
def build_char_vocab_zhe(df, L=40, R=40, min_freq=1):
    from collections import Counter
    cnt = Counter()
    for a,b in zip(df["left_ctx"], df["right_ctx"]):
        if isinstance(a,str): cnt.update(list(a[-L:]))
        if isinstance(b,str): cnt.update(list(b[:R]))
    cnt.update(["著"])  # center
    itos = ["<pad>","<unk>"]
    for ch, c in cnt.items():
        if c >= min_freq:
            itos.append(ch)
    stoi = {ch:i for i,ch in enumerate(itos)}
    return stoi, itos

# Triggered side features
_NEG_CHS = set(list("不未無无没沒"))
_ASP_CHS = set(list("了過过着著呢"))
def trig_feats_zhe(left, right):
    s = (left or "") + (right or "")
    neg = int(any(ch in s for ch in _NEG_CHS))
    asp = int(any(ch in s for ch in _ASP_CHS))
    # whether the right neighbor immediately contain words describing tone or demeanor (using words like "呢/着/了" to maintain consistency with ZAI's approach)
    rn  = int((right or "")[:1] in ("呢","着","著","了"))
    return np.array([neg, asp, rn], dtype=np.float32)

def encode_window_zhe(left, right, stoi, L=40, R=40):
    l = list(left[-L:]) if isinstance(left,str) else []
    r = list(right[:R]) if isinstance(right,str) else []
    seq = l + ["著"] + r   # The center character, ZHE
    ids = [stoi.get(ch, stoi["<unk>"]) for ch in seq]
    MAX_LEN = L + 1 + R
    if len(ids) < MAX_LEN: ids += [stoi["<pad>"]] * (MAX_LEN - len(ids))
    return np.array(ids[:MAX_LEN], dtype=np.int64)

# ========== Dataset: No cond_sub; layer features optional ==========
class ZheDataset(Dataset):
    def __init__(self, df, idxs, stoi):
        self.df = df
        self.idxs = np.array(idxs)
        self.stoi = stoi
        self.labels = ZHE_LABELS_FULL
        self.lab2id = {l:i for i,l in enumerate(self.labels)}
        self.L = CFG["L_CTX"]; self.R = CFG["R_CTX"]

    def __len__(self): return len(self.idxs)

    def __getitem__(self, i):
        j = int(self.idxs[i])
        row = self.df.iloc[j]
        left  = row["left_ctx"] or ""
        right = row["right_ctx"] or ""

        x_ids = encode_window_zhe(left, right, self.stoi, self.L, self.R)
        side3 = trig_feats_zhe(left, right)

        # —— Only layer switch, ZHE has no cond_sub —— #
        if CFG.get("USE_LAYER_FEAT", False):
            try:
                layer_id = int(row["layer"])
            except Exception:
                layer_id = 0
        else:
            layer_id = 0

        y = self.lab2id[row["label_silver"]]
        return {
            "x_ids": torch.tensor(x_ids, dtype=torch.long),
            "layer": torch.tensor(layer_id, dtype=torch.long),
            "cond":  torch.tensor(0, dtype=torch.long),  # only a placeholder
            "side3": torch.tensor(side3, dtype=torch.float32),
            "y":     torch.tensor(y, dtype=torch.long),
        }

# ========== Segmentation & Loader ==========
df_trainable_zhe, idx_tr, idx_dv, idx_te = split_by_book_zhe(df_silver_zhe, test_size=0.15, dev_size=0.15, seed=SEED)

# The vocabulary is constructed based on the training set
stoi_zhe, itos_zhe = build_char_vocab_zhe(df_trainable_zhe.iloc[idx_tr], L=CFG["L_CTX"], R=CFG["R_CTX"])
VOCAB_SIZE_ZHE = len(itos_zhe)

train_ds = ZheDataset(df_trainable_zhe, idx_tr, stoi_zhe)
dev_ds   = ZheDataset(df_trainable_zhe, idx_dv, stoi_zhe)
test_ds  = ZheDataset(df_trainable_zhe, idx_te, stoi_zhe)

train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=2, pin_memory=False)
dev_loader   = DataLoader(dev_ds,   batch_size=64, shuffle=False, num_workers=2, pin_memory=False)
test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=2, pin_memory=False)

print(f"[ZHE vocab] size={VOCAB_SIZE_ZHE}  labels={train_ds.labels}  features: layer={CFG.get('USE_LAYER_FEAT', False)} (no cond_sub)")

# ==== 4) Model & Training (close the cond branch) ====
import torch.nn as nn
from sklearn.metrics import f1_score, classification_report, confusion_matrix

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

EMB_DIM = 128
CNN_KS  = [2,3,4]
CNN_CH  = 128
LSTM_H  = 128
SIDE_EMB_DIM = 8
LR = 3e-4
WD = 1e-2
EPOCHS = 30
PATIENCE = 6
LABEL_SMOOTH = 0.1

class AttnPool(nn.Module):
    def __init__(self, in_dim): super().__init__(); self.w = nn.Linear(in_dim, 1)
    def forward(self, hiddens, mask=None):
        logits = self.w(hiddens).squeeze(-1)
        if mask is not None: logits = logits.masked_fill(~mask, -1e9)
        attn = torch.softmax(logits, dim=-1)
        out = torch.bmm(attn.unsqueeze(1), hiddens).squeeze(1)
        return out, attn

class CharCNN_BiLSTM_Att(nn.Module):
    def __init__(self, vocab_size, num_labels, have_cond=False):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, EMB_DIM, padding_idx=0)
        self.emb_drop = nn.Dropout(0.1)
        self.convs = nn.ModuleList([nn.Conv1d(EMB_DIM, CNN_CH, k) for k in CNN_KS])

        self.lstm = nn.LSTM(EMB_DIM, LSTM_H, num_layers=1, batch_first=True, bidirectional=True)
        self.lstm_drop = nn.Dropout(0.2)
        self.attn = AttnPool(LSTM_H*2)

        self.layer_emb = nn.Embedding(6, SIDE_EMB_DIM)   # layer 0..5
        self.have_cond = False                           # ZHE Fixed without cond_sub
        cnn_out = CNN_CH * len(CNN_KS)
        lstm_out = LSTM_H * 2
        side_dim = SIDE_EMB_DIM + 3  # +3 = side3
        in_dim = cnn_out + lstm_out + side_dim

        self.fc1 = nn.Linear(in_dim, 256)
        self.drop = nn.Dropout(0.3)
        self.fc2 = nn.Linear(256, num_labels)

    def forward(self, x_ids, layer_id, cond_id, side3):
        x = self.emb(x_ids); x = self.emb_drop(x)
        xc = x.transpose(1,2)
        cnn_feats = [torch.max(torch.relu(conv(xc)), dim=-1)[0] for conv in self.convs]
        cnn_vec = torch.cat(cnn_feats, dim=1)

        lstm_out, _ = self.lstm(x)
        lstm_out = self.lstm_drop(lstm_out)
        lstm_vec, _ = self.attn(lstm_out)

        layer_vec = self.layer_emb(layer_id)
        side_vec = torch.cat([layer_vec, side3], dim=1)   # No cond vector
        feat = torch.cat([cnn_vec, lstm_vec, side_vec], dim=1)

        h = torch.relu(self.fc1(feat)); h = self.drop(h)
        logits = self.fc2(h)
        return logits

def build_class_weights(df, idxs, labels):
    lab2id = {l:i for i,l in enumerate(labels)}
    y = df.iloc[idxs]["label_silver"].map(lab2id).values
    counts = np.bincount(y, minlength=len(labels))
    w = counts.sum() / (counts + 1e-6)
    w = w / w.mean()
    return torch.tensor(w, dtype=torch.float32, device=DEVICE)

def run_epoch(model, loader, criterion, optimizer=None):
    train_mode = optimizer is not None
    model.train(train_mode)
    total_loss, preds, golds = 0.0, [], []
    for batch in loader:
        x_ids = batch["x_ids"].to(DEVICE)
        layer = batch["layer"].to(DEVICE)
        cond  = batch["cond"].to(DEVICE)  # placeholder
        side3 = batch["side3"].to(DEVICE)
        y     = batch["y"].to(DEVICE)

        logits = model(x_ids, layer, cond, side3)
        loss = criterion(logits, y)

        if train_mode:
            optimizer.zero_grad(); loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        total_loss += loss.item() * len(x_ids)
        preds.append(logits.argmax(dim=1).detach().cpu().numpy())
        golds.append(y.detach().cpu().numpy())

    preds = np.concatenate(preds); golds = np.concatenate(golds)
    f1 = f1_score(golds, preds, average="macro")
    return total_loss / len(loader.dataset), f1, preds, golds

# ==== Construction/Training (M0/M1) ====
labels_space = train_ds.labels
model = CharCNN_BiLSTM_Att(VOCAB_SIZE_ZHE, len(labels_space), have_cond=False).to(DEVICE)

class_weights = build_class_weights(df_trainable_zhe, idx_tr, labels_space)
criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTH)
optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)

best_f1, best_state, pat = -1.0, None, PATIENCE
for ep in range(1, EPOCHS+1):
    tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, criterion, optimizer)
    dv_loss, dv_f1, dv_pred, dv_gold = run_epoch(model, dev_loader, criterion, optimizer=None)
    print(f"[ZHE] Ep{ep:02d} | tr {tr_loss:.4f}/{tr_f1:.4f} | dv {dv_loss:.4f}/{dv_f1:.4f}")
    if dv_f1 > best_f1:
        best_f1, pat = dv_f1, PATIENCE
        best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}
        print(f"  ✓ new best dev macro-F1 = {best_f1:.4f}")
    else:
        pat -= 1
        if pat == 0:
            print("  ✗ early stop."); break

# —— Test Set Evaluation
if best_state is not None:
    model.load_state_dict(best_state)

_, _, te_pred, te_gold = run_epoch(model, test_loader, criterion, optimizer=None)
print("[ZHE] test report:\n", classification_report(te_gold, te_pred, target_names=labels_space, digits=3))
print("[ZHE] confusion:\n", confusion_matrix(te_gold, te_pred))

# —— Save weights (ZHE prefix)
if CFG["SAVE_WEIGHTS"]:
    out_dir = Path("/content/weights_zhe")
    out_dir.mkdir(parents=True, exist_ok=True)
    tag = "ALL"  # training across all classes.
    weight_path = out_dir / f"charcnn_bilstm_att_{tag}.pt"
    torch.save({
        "state_dict": model.state_dict(),
        "labels": labels_space,
        "stoi": stoi_zhe,
        "itos": itos_zhe,
        "cfg": {k:v for k,v in CFG.items() if k in ["L_CTX","R_CTX","USE_LAYER_FEAT"]},
    }, weight_path)
    print(f"[save] ZHE weights -> {weight_path}")

# — Easily run multiple configurations (M0/M1; M2 is equivalent to M1, placeholder to align with the existing scripts) —
def train_one_zhe(tag: str, use_layer: bool):
    CFG["USE_LAYER_FEAT"] = use_layer

    stoi, itos = build_char_vocab_zhe(df_trainable_zhe.iloc[idx_tr], L=CFG["L_CTX"], R=CFG["R_CTX"])
    train_ds = ZheDataset(df_trainable_zhe, idx_tr, stoi)
    dev_ds   = ZheDataset(df_trainable_zhe, idx_dv, stoi)
    test_ds  = ZheDataset(df_trainable_zhe, idx_te, stoi)
    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=2, pin_memory=False)
    dev_loader   = DataLoader(dev_ds,   batch_size=64, shuffle=False, num_workers=2, pin_memory=False)
    test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=2, pin_memory=False)

    labels_space = train_ds.labels
    model = CharCNN_BiLSTM_Att(len(itos), len(labels_space), have_cond=False).to(DEVICE)

    class_weights = build_class_weights(df_trainable_zhe, idx_tr, labels_space)
    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTH)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)

    best_f1, best_state, pat = -1.0, None, PATIENCE
    for ep in range(1, EPOCHS+1):
        tr_loss, tr_f1, _, _ = run_epoch(model, train_loader, criterion, optimizer)
        dv_loss, dv_f1, dv_pred, dv_gold = run_epoch(model, dev_loader, criterion, optimizer=None)
        print(f"[{tag}|ZHE] Ep{ep:02d} | tr {tr_loss:.4f}/{tr_f1:.4f} | dv {dv_loss:.4f}/{dv_f1:.4f}")
        if dv_f1 > best_f1:
            best_f1, pat = dv_f1, PATIENCE
            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}
            print(f"  ✓ new best dev macro-F1={best_f1:.4f}")
        else:
            pat -= 1
            if pat==0:
                print("  ✗ early stop"); break

    if best_state is not None:
        model.load_state_dict(best_state)

    _, _, te_pred, te_gold = run_epoch(model, test_loader, criterion, optimizer=None)
    print(f"[{tag}|ZHE] test report:\n", classification_report(te_gold, te_pred, target_names=labels_space, digits=3))
    print(f"[{tag}|ZHE] confusion:\n", confusion_matrix(te_gold, te_pred))

    if CFG["SAVE_WEIGHTS"]:
        out_dir = Path("/content/weights_zhe"); out_dir.mkdir(parents=True, exist_ok=True)
        weight_path = out_dir / f"charcnn_bilstm_att_{tag}.pt"
        torch.save({
            "state_dict": model.state_dict(),
            "labels": labels_space,
            "stoi": stoi,
            "itos": itos,
            "cfg": {k:v for k,v in CFG.items() if k in ["L_CTX","R_CTX","USE_LAYER_FEAT"]},
        }, weight_path)
        print(f"[save] -> {weight_path}")

# M0：context-only
train_one_zhe(tag="M0_ctx_only", use_layer=False)

# M1：+layer
train_one_zhe(tag="M1_ctx+layer", use_layer=True)

# ==== gold sampling for 1200 (FAST & Robust) ====

# =========================
# ZHE Gold Label Sampling (No Scoring; 3-on-1 strategy)
# =========================
# Objectives:
# 1) No scoring; sampling only by quota/proportion/constraint;
# 2) Column order: left_ctx, zhe, right_ctx;
# 3) Century mapping comes from two guidelines (MC/EMoC), only "Arabic numerals" are used, no UNK is mapped;
# 4) Does not depend on __source_file; uses stable row_id;
# 5) Three strategies combined: label → (epoch × layer × century) → trigger, with "upper limit per book", lightweight randomness;
# 6) When insufficient, "gradually relax" to reach ≥ TOTAL_TARGET;
# 7) Export final (label template) and audit (audit).

import re, math, json, random

rng = np.random.default_rng(42)

# =========================
# 0) Parameters
# =========================
# If there is df_silver_zhe in memory, one can set USE_INMEM_SILVER=True; otherwise, read it from CSV
USE_INMEM_SILVER = True
SILVER_CSV = "/data/NN training/ZHE_silver/df_silver_zhe.csv"

OUT_DIR = Path("/data/NN training/ZHE_gold")
OUT_DIR.mkdir(parents=True, exist_ok=True)

TOTAL_TARGET = 1200
# Total quota for the four categories (the sum should equal TOTAL_TARGET)
QUOTA_BY_LABEL = {
    "Zh-Dur":  450,
    "Zh-Dur?": 300,
    "Zh-LexV": 270,
    "Zh-P":    180,
}
assert sum(QUOTA_BY_LABEL.values()) == TOTAL_TARGET

# Trigger (Target percentage of A-D; E automatically fills in)
TRIGGER_TARGET = {
    "A": 0.25,   # Chinese character + 著 + Chinese character (not “所…著”)
    "B": 0.25,   # Location/Directional Sig.
    "C": 0.25,   # Negation
    "D": 0.10,   # explicit(著(Di/T))
    # E = 1 - The sum of the above
}

# Maximum limit per book (can be relaxed for boundary classes)
PER_BOOK_CAP = {
    "Zh-Dur":  6,
    "Zh-Dur?": 10,
    "Zh-LexV": 6,
    "Zh-P":    6,
}

# =========================
# 1) Tools: Strictly read using a pipeline table only during the "Gold Standard Stage" → book → century (number)
# =========================

def _clean_header_local(cols):
    return [str(c).replace("*","").strip().strip("|").strip() for c in cols]

def _read_guideline_pipe_clean(path: str) -> pd.DataFrame:
    # Force reading by pipe table and clear dirty "|" cells & separators
    df = pd.read_csv(Path(path), sep="|", engine="python", dtype=str, encoding="utf-8-sig")
    df.columns = _clean_header_local(df.columns)
    # Remove leading and trailing empty columns generated during parsing
    df = df.loc[:, [c for c in df.columns if c and not c.isspace()]]
    # Treat isolated "|" characters in a cell as missing.
    df = df.replace("|", pd.NA)
    # Discard all empty columns and rows consisting entirely of separators/blanks.
    df = df.dropna(axis=1, how="all")
    mask = pd.Series([True]*len(df))
    for col in df.columns:
        s = df[col].astype(str).str.strip()
        mask &= ~s.str.fullmatch(r"-{3,}")
    df = df[mask].copy()
    return df

def _normalize_book(raw: str) -> str:
    if not isinstance(raw, str):
        return ""
    return raw.strip().split("/")[0].strip()

def _normalize_book_like(x: str) -> str:
    try:
        return _normalize_book(x)  # Reuse the existing logic
    except NameError:
        if not isinstance(x,str): return ""
        return x.strip().split("/")[0].strip()

def _extract_century_number(s: str) -> str:
    """Extract the first Arabic numeral from the left in the range  '16 世纪' / '16世纪' / '16-17 世纪'"""
    if not isinstance(s,str): return ""
    m = re.search(r"(\d+)", s)
    return m.group(1) if m else ""

def _build_book2century_strict(mc_path: str, emoc_path: str) -> dict:
    b2c = {}
    for pth in [mc_path, emoc_path]:
        g = _read_guideline_pipe_clean(pth)
        # find the columns
        cols = {c:c for c in g.columns}
        def _find(cands):
            for k in cands:
                if k in cols: return k
            return None
        name_key = _find(["书名","書名","title","Title"])
        cent_key = _find(["世纪","世紀","century","Century"])
        if not (name_key and cent_key):
            continue
        sub = g[[name_key, cent_key]].dropna()
        sub["book_key"] = sub[name_key].astype(str).apply(_normalize_book_like)
        sub["century_num"] = sub[cent_key].astype(str).apply(_extract_century_number)
        # Only numbers are retained
        sub = sub[sub["century_num"]!=""].drop_duplicates(subset=["book_key"], keep="first")
        b2c.update(dict(zip(sub["book_key"], sub["century_num"])))
    return b2c

# Construct a strict mapping (only numbers are needed)
book2century_strict = _build_book2century_strict(GUIDE_MC, GUIDE_EMOC)

# =========================
# 2) Prepare the silver standard (read in or use the memory variable)
# =========================
if USE_INMEM_SILVER:
    try:
        df_silver_zhe  # If it already exists, do not report an error.
    except NameError:
        raise RuntimeError("USE_INMEM_SILVER=True 但未发现 df_silver_zhe 变量。改为 False 或先构建该变量。")
else:
    df_silver_zhe = pd.read_csv(SILVER_CSV, dtype=str).fillna("")

# Basic field fallback
for col in ["epoch","layer","book_id","left_ctx","right_ctx","zhe","condition","label","label_silver"]:
    if col not in df_silver_zhe.columns:
        df_silver_zhe[col] = ""

# =========================
# 3) Trigger + base table integer (ZHE in the middle; century mapping; stable row_id)
# =========================
HAN = r"[\u4e00-\u9fff]"
LOC = r"[上下内中前後东西南北於于在間里裏內外旁上中下前后左右內間之]"
NEG_SET = set(list("不未無无没沒"))

def trig_flags(row) -> dict:
    left = str(row.get("left_ctx","") or "")
    right= str(row.get("right_ctx","") or "")
    zhe  = str(row.get("zhe","") or "")
    cond = str(row.get("condition","") or "")
    full = left + zhe + right

    A = (re.search(fr"(?<!所){HAN}著{HAN}", full) is not None)                 # Character+著+Character，but not“所…著”
    B = (re.search(fr"{LOC}", full) is not None)                               # location/directional sig.
    C = any(ch in NEG_SET for ch in full)                                      # negation
    D = bool(re.search(r"著\((Di|T)\)", cond) or "著(Di/T)" in cond or re.search(r"著\((Di|T)\)", full))  # explicit evidence
    E = not (A or B or C or D)

    return {"A":A, "B":B, "C":C, "D":D, "E":E}

def prepare_base(df_silver_zhe: pd.DataFrame, book2century: dict) -> pd.DataFrame:
    base = df_silver_zhe.copy().reset_index(drop=True)

    # Stable row_id (replaces any approach that depends on filename/original index)
    base["row_id"] = np.arange(len(base), dtype=int)

    #Century mapping (strictly digital version), UNK not hit
    base["century"] = base["book_id"].astype(str).apply(_normalize_book_like).map(book2century_strict).fillna("UNK")

    # Trigger
    tflags = base.apply(trig_flags, axis=1, result_type="expand")
    for k in ["A","B","C","D","E"]:
        base[f"trig_{k}"] = tflags[k].astype(bool)

    # The layer number is int
    base["layer"] = pd.to_numeric(base.get("layer", 0), errors="coerce").fillna(0).astype(int)

    # Column order: ZHE placed in the middle
    cols_order = [
        "epoch","layer","book_id","century",
        "left_ctx","zhe","right_ctx",
        "condition","label","label_silver",
        "trig_A","trig_B","trig_C","trig_D","trig_E",
        "row_id",
    ]
    for c in cols_order:
        if c not in base.columns:
            base[c] = ""
    base = base[cols_order].copy()
    return base

base = prepare_base(df_silver_zhe, book2century_strict)

# Report on books that cannot map centuries (optional)
unk_books = base.loc[base["century"]=="UNK","book_id"].unique().tolist()
print(f"[INFO] 无法映射世纪的书本数量：{len(unk_books)}（保留为 UNK，后续可人工兜底）")

# =========================
# 4) Sampling assistance
# =========================
def proportional_split(total: int, counts: pd.Series) -> dict:
    counts = counts.astype(int)
    if counts.sum() == 0:
        return {k:0 for k in counts.index}
    raw = (counts / counts.sum() * total).values
    flo = np.floor(raw).astype(int)
    remain = int(total - flo.sum())
    frac = raw - flo
    order = np.argsort(-frac)
    flo[order[:remain]] += 1
    return {k:int(v) for k,v in zip(counts.index, flo)}

rng = np.random.default_rng(42)

def _cap_by_book_random(df_bucket: pd.DataFrame, cap: int) -> pd.DataFrame:
    """First, shuffle the entire list, then sort by book head (cap) to avoid oversampling of any particular book."""
    if df_bucket.empty or cap <= 0:
        return df_bucket.iloc[[]]
    g = (df_bucket.sample(frac=1.0, random_state=42)
                    .groupby("book_id", group_keys=False)
                    .head(cap))
    return g

def _take_n_random(df_pool: pd.DataFrame, n: int) -> pd.DataFrame:
    if df_pool.empty or n <= 0:
        return df_pool.iloc[[]]
    if len(df_pool) <= n:
        return df_pool.sample(frac=1.0, random_state=42)
    return df_pool.sample(n=n, random_state=42)

# =========================
# 5) Stratified random sampling（label→epoch×layer×century→trigger）
# =========================
audit_rows = []
picked_all = []

for label, total_label in QUOTA_BY_LABEL.items():
    sub = base[base["label_silver"]==label].copy()
    if sub.empty:
        audit_rows.append({"label":label, "stage":"none", "need":total_label, "pool":0, "picked":0})
        continue

    # Allocate the quota first according to the ratio of epoch × layer × century.
    key_cols = ["epoch","layer","century"]
    counts_k = sub.groupby(key_cols).size()
    q_label2k = proportional_split(total_label, counts_k)

    for (e,l,c), need_k in q_label2k.items():
        sub_k = sub[(sub["epoch"]==e) & (sub["layer"]==l) & (sub["century"]==c)].copy()
        if need_k <= 0 or sub_k.empty:
            audit_rows.append({"label":label,"epoch":e,"layer":l,"century":c,
                               "stage":"k", "need":need_k, "pool":len(sub_k), "picked":0})
            continue

        # The target quantity is coarsely divided according to trigger (the sum of the four categories is less than or equal to need_k, and the remainder is assigned to E)
        tA = int(round(need_k * TRIGGER_TARGET.get("A", 0.0)))
        tB = int(round(need_k * TRIGGER_TARGET.get("B", 0.0)))
        tC = int(round(need_k * TRIGGER_TARGET.get("C", 0.0)))
        tD = int(round(need_k * TRIGGER_TARGET.get("D", 0.0)))
        taken = tA + tB + tC + tD
        tE = max(0, need_k - taken)
        tgt = {"A":tA, "B":tB, "C":tC, "D":tD, "E":tE}

        chosen_parts = []

        # Trigger-by-trigger: prune the pool to the "maximum limit per book" at first, then randomly select need_T
        for T in ["A","B","C","D","E"]:
            need_T = tgt[T]
            if need_T <= 0:
                audit_rows.append({"label":label,"epoch":e,"layer":l,"century":c,
                                   "stage":f"T{T}", "need":need_T, "pool":0, "picked":0})
                continue

            pool_T = sub_k[sub_k[f"trig_{T}"]==True]
            pool_T_cap = _cap_by_book_random(pool_T, PER_BOOK_CAP[label])
            take_T = min(need_T, len(pool_T_cap))
            part = _take_n_random(pool_T_cap, take_T)
            chosen_parts.append(part)

            audit_rows.append({"label":label,"epoch":e,"layer":l,"century":c,
                               "stage":f"T{T}", "need":need_T, "pool":len(pool_T_cap), "picked":len(part)})

        if chosen_parts:
            picked_all.append(pd.concat(chosen_parts, ignore_index=True))

# =========================
# 6) Gradually increase the padding size (ensuring ≥ TOTAL_TARGET), and use row_id for stable deduplication
# =========================
if picked_all:
    selected = pd.concat(picked_all, ignore_index=True)
else:
    selected = base.iloc[[]].copy()

# —— A) First, assign the "raw RID" to each row and calculate the remaining pool accordingly
# (The row_id has already been added to prepare_base above, so we'll use it directly here.)
def fast_top_up_at_least(selected: pd.DataFrame, base: pd.DataFrame, target_min: int) -> pd.DataFrame:
    if len(selected) >= target_min:
        return selected

    def _cap_pool(pool, lab, already_by_book, cap_map):
        chunks = []
        for bk, g in pool.groupby("book_id"):
            cur = int(already_by_book.get(bk, 0))
            cap_left = max(0, cap_map.get(lab, 6) - cur)
            if cap_left > 0:
                chunks.append(g.sample(frac=1.0, random_state=42).head(cap_left))
        return pd.concat(chunks, ignore_index=True) if chunks else pool.iloc[[]]

    # Step 0: Prepare various views
    picked_rids = set(selected["row_id"].tolist())
    remain_all  = base[~base["row_id"].isin(picked_rids)].copy()

    # The target difference is amortized according to the "remaining label percentage"
    need_total = target_min - len(selected)
    need_by_lab = proportional_split(need_total, remain_all["label_silver"].value_counts())

    # Gradually relax the restrictions: cap → cap + 2 → cap + 4 → no cap; finally, completely arbitrary (ignoring triggers)
    CAP0 = {"Zh-Dur":6,"Zh-Dur?":10,"Zh-LexV":6,"Zh-P":6}
    CAP1 = {k:v+2 for k,v in CAP0.items()}
    CAP2 = {k:v+4 for k,v in CAP0.items()}

    stages = [
        ("label+cap0+any_trigger", CAP0, True),
        ("label+cap1+any_trigger", CAP1, True),
        ("label+cap2+any_trigger", CAP2, True),
        ("label+NO_CAP+any_trigger", None, True),
        ("label+NO_CAP+no_trigger", None, False),  # Last resort: Ignore triggers
    ]

    cur_selected = selected.copy()

    for stage_name, cap_map, respect_trigger in stages:
        if len(cur_selected) >= target_min:
            break

        # Recalculate the remainder and difference (update in each round)
        picked_rids = set(cur_selected["row_id"].tolist())
        remain_all  = base[~base["row_id"].isin(picked_rids)].copy()
        need_total  = target_min - len(cur_selected)
        need_by_lab = proportional_split(need_total, remain_all["label_silver"].value_counts())

        stage_pick = []

        for lab, need_lab in need_by_lab.items():
            if need_lab <= 0:
                continue
            pool_l = remain_all[remain_all["label_silver"]==lab].copy()

            # Whether to respect triggers: True = any trigger; False = ignore triggers (completely arbitrary)
            # To improve recall, we can relax the trigger restrictions during the replenishment phase (it is already an "arbitrary trigger" pool)
            if pool_l.empty:
                continue

            # Calculate the distribution of currently selected books
            cur_by_book = cur_selected[cur_selected["label_silver"]==lab].groupby("book_id").size()

            # Maximum number of books per application (no limit if cap_map=None)
            if cap_map is not None:
                pool_l = _cap_pool(pool_l, lab, cur_by_book, cap_map)

            if pool_l.empty:
                continue

            take = min(need_lab, len(pool_l))
            part = pool_l.sample(n=take, random_state=42)
            stage_pick.append(part)

        if stage_pick:
            added = pd.concat(stage_pick, ignore_index=True)
            cur_selected = pd.concat([cur_selected, added], ignore_index=True)
            print(f"[topup] stage={stage_name}  +{len(added)}  total={len(cur_selected)}")

    return cur_selected

selected = fast_top_up_at_least(selected, base, TOTAL_TARGET)

# — Export (the column order remains the same as the base order, avoiding expensive operations) —
audit = pd.DataFrame(audit_rows)
audit = audit[["label","epoch","layer","century","stage","need","pool","picked"]].fillna({"epoch":"","century":"","stage":""})

out_final = OUT_DIR / "zhe_gold_batch01_final.csv"
out_audit = OUT_DIR / "zhe_gold_batch01_audit.csv"
selected.to_csv(out_final, index=False, encoding="utf-8-sig")
audit.to_csv(out_audit, index=False, encoding="utf-8-sig")

print(f"[FAST done] gold final ≥{TOTAL_TARGET}: {out_final}  (n={len(selected)})")
print(f"[FAST done] audit: {out_audit}")
print("label 分布：\n", selected["label_silver"].value_counts(), "\n")

# === 10% double-check sampling + marked column ===
def read_csv_smart(path, try_encodings=("utf-8", "utf-8-sig", "gb18030", "cp936", "big5", "latin1")):
    last_err = None
    for enc in try_encodings:
        try:
            return pd.read_csv(path, dtype=str, encoding=enc)
        except Exception as e:
            last_err = e
            continue
    raise last_err

in_path   = "data/NN training/ZHE_gold/zhe_gold_batch01_final_annot_compl.csv"
out_full  = "data/NN training/ZHE_gold/zhe_gold_batch01_final_annot_with_qc.csv"
    # Full volume + QC marking

df = read_csv_smart(in_path).fillna("")

# —— Randomly select 10% (at least 1 item), repeats allowed ——
rng = np.random.default_rng(42)
n = max(1, int(round(len(df) * 0.10)))
sample_idx = df.sample(n=n, random_state=42).index  # Or：df.sample(frac=0.10, random_state=42).index

# —— Added QC marked column ——
QC_COL = "qc_10pct"
df[QC_COL] = ""
df.loc[sample_idx, QC_COL] = "REVIEW"

# —— Save (UTF-8 with BOM, compatible with Excel) ——
df.to_csv(out_full, index=False, encoding="utf-8-sig")

print(f"总计 {len(df)} 条；QC 样本 {len(sample_idx)} 条。")
print("已保存：")
print("  全量+标记：", out_full)

# === Global Determinism Lock ===
import os, random, numpy as np
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"  # More stable with CUDA; no error message without it
random.seed(SEED); np.random.seed(SEED)

try:
    import torch
    torch.manual_seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    try: torch.use_deterministic_algorithms(True)
    except Exception: pass
except Exception:
    pass


# ===== A) CONFIG & UTILS =====
import re, textwrap, numpy as np, pandas as pd
from pathlib import Path

SEED = 42
np.random.seed(SEED)

# Path: Modify these 2 lines as needed
GOLD_DIR = Path("data/NN training/ZHE_gold")
IN_ANNOT_CSV = GOLD_DIR / "zhe_gold_batch01_final_annot_compl.csv"

GOLD_DIR.mkdir(parents=True, exist_ok=True)

# Output file name (fixed)
OUT_GOLD_V1  = GOLD_DIR / "zhe_gold_GOLD_v1.csv"
OUT_TEST_TXT = GOLD_DIR / "zhe_gold_TEST_v1.txt"
OUT_TRAIN    = GOLD_DIR / "zhe_gold_TRAIN_v1.csv"
OUT_DEV      = GOLD_DIR / "zhe_gold_DEV_v1.csv"
DATA_CARD_MD = GOLD_DIR / "DATA_CARD.md"

# full set of labels
LABELS = ["Zh-LexV","Zh-P","Zh-Dur?","Zh-Dur"]

def read_csv_smart(path, encs=("utf-8-sig", "gb18030")):
    last = None
    for enc in encs:
        try:
            return pd.read_csv(path, dtype=str, encoding=enc)
        except Exception as e:
            last = e
            continue
    raise last

# Triggers (used for sampling/report stratification)
HAN = r"[\u4e00-\u9fff]"
LOC = r"[上下内中前後东西南北於于在間里裏內外旁上中下前后左右內間之]"
NEG_SET = set(list("不未無无没沒"))
def mark_triggers(row):
    left = str(row.get("left_ctx","") or "")
    right= str(row.get("right_ctx","") or "")
    zhe  = str(row.get("zhe","") or "")
    full = left + zhe + right
    A = (re.search(fr"(?<!所){HAN}著{HAN}", full) is not None)
    B = (re.search(fr"{LOC}", full) is not None)
    C = any(ch in NEG_SET for ch in full)
    D = bool(re.search(r"著\((Di|T)\)", full))   # explicit (Di/T) in the texts
    E = not (A or B or C or D)
    return pd.Series({"trig_A":A,"trig_B":B,"trig_C":C,"trig_D":D,"trig_E":E})

# ===== B) QC CLOSE & EXPORT GOLD_v1 =====
need_cols = ["epoch","layer","book_id","century","left_ctx","zhe","right_ctx","label_gold"]
df = read_csv_smart(IN_ANNOT_CSV).fillna("")
miss = [c for c in need_cols if c not in df.columns]
if miss:
    raise ValueError(f"人工标注文件缺列：{miss}")

# 1) Normalize tags: remove whitespace + minor mapping correction
LABELS = ["Zh-LexV","Zh-P","Zh-Dur?","Zh-Dur"]
FIX_MAP = {
    "Zh-LexN": "Zh-LexV",   # Your example: N -> V
    "Zh-lexv": "Zh-LexV",
    "zh-lexv": "Zh-LexV",
    "ZHP": "Zh-P",
}
df["label_gold"] = df["label_gold"].astype(str).str.strip().replace(FIX_MAP)

# 2) if there are Illegal labels, they will be identified and discarded, ignored on the spot, but to print an alert
bad_mask = ~df["label_gold"].isin(LABELS)
if bad_mask.any():
    print("[warn] 将忽略以下非法标签（数量）并从金标集中移除：")
    print(df.loc[bad_mask, "label_gold"].value_counts())
    df = df.loc[~bad_mask].copy()

# 3) Stable row_id (for easy tracking)
df = df.reset_index(drop=True)
df["row_id"] = np.arange(len(df), dtype=int)

# 4) Trigger (for reporting/sampling stratification only, not for training features)
trigs = df.apply(mark_triggers, axis=1)
df = pd.concat([df, trigs], axis=1)

# 5) Export GOLD_v1 (keeping row_id)
gold_cols = ["epoch","layer","book_id","century","left_ctx","zhe","right_ctx","label_gold","row_id"]
df_gold = df[gold_cols].copy()
df_gold.to_csv(OUT_GOLD_V1, index=False, encoding="utf-8-sig")
print(f"[GOLD_v1] saved: {OUT_GOLD_V1}  n={len(df_gold)}")
print(df_gold["label_gold"].value_counts())

# ===== C-lite) FREEZE A LIGHT TEST (LABEL-ONLY) =====
N_TEST = 240
TEST_PER_BOOK_CAP = 2
SEED = 42

rng = np.random.default_rng(SEED)
df_all = df_gold.copy()

# # Average value per label
per_lab = {lab: N_TEST // len(LABELS) for lab in LABELS}
for i in range(N_TEST - sum(per_lab.values())):
    per_lab[LABELS[i]] += 1

test_idx = []
for lab, need in per_lab.items():
    sub = df_all[df_all["label_gold"]==lab].copy()
    if sub.empty or need<=0:
        continue
    # First shuffle the samples, then take samples according to the book's quota
    sub = sub.sample(frac=1.0, random_state=SEED)
    capped = (sub.groupby("book_id", group_keys=False)
                 .head(TEST_PER_BOOK_CAP))
    take = capped if len(capped) <= need else capped.sample(n=need, random_state=SEED)
    test_idx.extend(take.index.tolist())

# Deduplication and order preservation; if the limit is exceeded, random truncation is performed
test_idx = list(dict.fromkeys(test_idx))
if len(test_idx) > N_TEST:
    test_idx = list(pd.Index(test_idx).sample(n=N_TEST, random_state=SEED))

df_test = df_all.loc[test_idx].copy()
# The remaining part are processed as train/dev
remain = df_all.drop(index=df_test.index)

from sklearn.model_selection import StratifiedGroupKFold
X = np.arange(len(remain))
y = remain["label_gold"].values
groups = remain["book_id"].values

sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)
train_idx, dev_idx = next(sgkf.split(X, y, groups=groups))
df_train = remain.iloc[train_idx].copy()
df_dev   = remain.iloc[dev_idx].copy()

# Export
with open(OUT_TEST_TXT, "w", encoding="utf-8") as f:
    for rid in df_test["row_id"]:
        f.write(str(int(rid))+"\n")
df_test.to_csv(GOLD_DIR/"zhe_gold_TEST_v1_preview.csv", index=False, encoding="utf-8-sig")
df_train.to_csv(OUT_TRAIN, index=False, encoding="utf-8-sig")
df_dev.to_csv(OUT_DEV, index=False, encoding="utf-8-sig")

print(f"[TEST-lite] frozen rows={len(df_test)}  (label-only stratified, per-book cap={TEST_PER_BOOK_CAP})")
print("test label dist:\n", df_test["label_gold"].value_counts())
print(f"[SPLIT] train={len(df_train)}  dev={len(df_dev)}")

import hashlib, json
def sha256(p):
    h = hashlib.sha256();
    with open(p,'rb') as f: h.update(f.read());
    return h.hexdigest()[:16]

card = {
  "seed": SEED,
  "files": {
    "TEST_v1.txt": sha256(OUT_TEST_TXT),
    "TRAIN_v1.csv": sha256(OUT_TRAIN),
    "DEV_v1.csv": sha256(OUT_DEV),
  }
}
with open(DATA_CARD_MD, "a", encoding="utf-8") as f:
    f.write("\n\n## Freeze Fingerprints\n" + json.dumps(card, ensure_ascii=False, indent=2))

# ===== D) TRAIN/DEV SPLIT =====
DEV_RATIO = 0.20

# Use the existing df_all / df_test in C-lite
remain = df_all[~df_all.index.isin(df_test.index)].copy()
books = (remain["book_id"].drop_duplicates()
         .sample(frac=1.0, random_state=SEED).tolist())

n_dev_books = max(1, int(round(len(books)*DEV_RATIO)))
dev_books = set(books[:n_dev_books])

df_dev   = remain[remain["book_id"].isin(dev_books)].copy()
df_train = remain[~remain["book_id"].isin(dev_books)].copy()

core_cols = ["epoch","layer","book_id","century","left_ctx","zhe","right_ctx","label_gold","row_id"]
df_train[core_cols].to_csv(OUT_TRAIN, index=False, encoding="utf-8-sig")
df_dev[core_cols].to_csv(OUT_DEV, index=False, encoding="utf-8-sig")

# A brief Data Card (also changed to df_all)
DATA_CARD_MD.write_text(textwrap.dedent(f"""
# ZHE GOLD_v1 Data Card
- seed: {SEED}
- GOLD_v1 size: {len(df_all)}
- TEST_v1 size: {len(df_test)}  (cap/book in test = {TEST_PER_BOOK_CAP})
- TRAIN size: {len(df_train)}; DEV size: {len(df_dev)}
- label dist (GOLD_v1):
{df_all['label_gold'].value_counts().to_string()}
"""), encoding="utf-8")

print(f"[SPLIT] train/dev saved → {OUT_TRAIN.name} ({len(df_train)}), {OUT_DEV.name} ({len(df_dev)})")

# ===== E) TRAIN BASELINES (gold_scratch & optional silver→gold) =====
# --- Security patch: Must be applied before importing torch ---
import os
# — This must be set before importing torch —
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"

os.environ["TORCHDYNAMO_DISABLE"] = "1"
os.environ["TORCHINDUCTOR_DISABLE"] = "1"
os.environ["PYTORCH_JIT"] = "0"

import random

import torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, confusion_matrix, f1_score

# — Device Branch: TPU uses determinism disabled; GPU uses cuBLAS determinism —
USE_TPU = ("COLAB_TPU_ADDR" in os.environ)

if USE_TPU:
    # XLA/TPU does not use cuBLAS, and forcing determinism results in errors/invalidity
    torch.use_deterministic_algorithms(False)
else:
    # CUDA/GPU: Enable determinism and use with cuBLAS workspaces
    torch.use_deterministic_algorithms(True)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Unified seed
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# If torch.compile exists, change it to no-op (preserve your original logic)
if hasattr(torch, "compile"):
    torch.compile = (lambda m, *args, **kwargs: m)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
L_CTX, R_CTX = 40, 40
BATCH = 64
EPOCHS = 12
PATIENCE = 4
LR, WD = 3e-4, 1e-2
LAB2ID = {l:i for i,l in enumerate(LABELS)}

def build_char_vocab(df):
    from collections import Counter
    cnt = Counter()
    for a,b in zip(df["left_ctx"], df["right_ctx"]):
        if isinstance(a,str): cnt.update(list(a[-L_CTX:]))
        if isinstance(b,str): cnt.update(list(b[:R_CTX]))
    cnt.update(["著"])
    # patch:
    chars = sorted(cnt.keys())                 # Or：sorted(cnt.items(), key=lambda x:(-x[1], x[0])) then take 'x[0]'
    itos = ["<pad>","<unk>"] + list(chars)
    stoi = {ch:i for i,ch in enumerate(itos)}
    return stoi, itos

def encode_window(left,right,stoi):
    l = list((left or "")[-L_CTX:])
    r = list((right or "")[:R_CTX])
    seq = l + ["著"] + r
    ids = [stoi.get(ch, stoi["<unk>"]) for ch in seq]
    MAX = L_CTX + 1 + R_CTX
    if len(ids) < MAX: ids += [stoi["<pad>"]] * (MAX-len(ids))
    return np.array(ids[:MAX], dtype=np.int64)

class GoldSet(Dataset):
    def __init__(self, df, stoi):
        self.df = df.reset_index(drop=True)
        self.stoi = stoi
    def __len__(self): return len(self.df)
    def __getitem__(self,i):
        r = self.df.iloc[i]
        x = encode_window(r["left_ctx"], r["right_ctx"], self.stoi)
        y = LAB2ID[r["label_gold"]]
        layer = int(pd.to_numeric(r["layer"], errors="coerce") or 0)
        return torch.tensor(x), torch.tensor(layer), torch.tensor(y)

class CharCNN_BiLSTM_Att(nn.Module):
    def __init__(self, vocab_size, nlab, emb=128, cnn_ch=128, lstm_h=128, side=8):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb, padding_idx=0)
        self.c2, self.c3, self.c4 = nn.Conv1d(emb, cnn_ch, 2), nn.Conv1d(emb, cnn_ch, 3), nn.Conv1d(emb, cnn_ch, 4)
        self.lstm = nn.LSTM(emb, lstm_h, batch_first=True, bidirectional=True)
        self.layer_emb = nn.Embedding(6, side)
        self.fc1 = nn.Linear(cnn_ch*3 + lstm_h*2 + side, 256)
        self.fc2 = nn.Linear(256, nlab)
        self.drop = nn.Dropout(0.3)
    def forward(self, x_ids, layer):
        x = self.emb(x_ids)              # [B,T,E]
        xc = x.transpose(1,2)            # [B,E,T]
        cmax = torch.cat([
            torch.max(torch.relu(self.c2(xc)), dim=-1)[0],
            torch.max(torch.relu(self.c3(xc)), dim=-1)[0],
            torch.max(torch.relu(self.c4(xc)), dim=-1)[0],
        ], dim=1)
        lstm_out,_ = self.lstm(x)        # [B,T,2H]
        att = torch.softmax((lstm_out**2).sum(-1), dim=-1).unsqueeze(1)
        lstm_vec = torch.bmm(att, lstm_out).squeeze(1)
        feat = torch.cat([cmax, lstm_vec, self.layer_emb(layer)], dim=1)
        h = torch.relu(self.fc1(feat))
        h = self.drop(h)
        return self.fc2(h)

def make_loaders(df_tr, df_dv, df_te):
    stoi,itos = build_char_vocab(df_tr)
    toSet = lambda D: GoldSet(D, stoi)
    g = torch.Generator(); g.manual_seed(SEED)
    Ltr = DataLoader(toSet(df_tr), batch_size=BATCH, shuffle=True,  generator=g, num_workers=0)
    Ldv = DataLoader(toSet(df_dv), batch_size=BATCH, shuffle=False, generator=g, num_workers=0)
    Lte = DataLoader(toSet(df_te), batch_size=BATCH, shuffle=False, generator=g, num_workers=0)
    return (stoi,itos), (Ltr,Ldv,Lte)

def train_eval(df_tr, df_dv, df_te, tag, warm_silver=None):
    EXP = GOLD_DIR / "experiments" / "GOLD_v1" / tag
    (EXP/"reports").mkdir(parents=True, exist_ok=True)

    (stoi,itos), (Ltr,Ldv,Lte) = make_loaders(df_tr, df_dv, df_te)
    model = CharCNN_BiLSTM_Att(len(itos), len(LABELS)).to(DEVICE)
    crit = nn.CrossEntropyLoss()
    opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)

    def run_epoch(loader, train=False):
        model.train(train)
        tot, pred, gold, prob = 0.0, [], [], []
        for x,layer,y in loader:
            x,layer,y = x.to(DEVICE), layer.to(DEVICE), y.to(DEVICE)
            with torch.set_grad_enabled(train):
                logits = model(x,layer)
                loss = crit(logits, y)
                if train:
                    opt.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()
            tot += loss.item()*len(x)
            pred.append(logits.detach().argmax(1).cpu().numpy())
            gold.append(y.cpu().numpy())
            prob.append(torch.softmax(logits,1).detach().cpu().numpy())
        pred = np.concatenate(pred); gold = np.concatenate(gold); prob = np.concatenate(prob)
        from sklearn.metrics import f1_score
        f1 = f1_score(gold, pred, average="macro")
        return tot/len(loader.dataset), f1, pred, gold, prob

    # (Optional) Silver Label warm-up (fast 3 epochs)
    if isinstance(warm_silver, pd.DataFrame) and len(warm_silver):
        sil = warm_silver.rename(columns={"label_silver":"label_gold"})
        (stoi,itos), (LtrS,LdvS,LteS) = make_loaders(sil, df_dv, df_te)
        model = CharCNN_BiLSTM_Att(len(itos), len(LABELS)).to(DEVICE)
        opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)
        for ep in range(1, 4):
            trL,trF,_,_,_ = run_epoch(LtrS, True)
            dvL,dvF,_,_,_ = run_epoch(LdvS, False)
            print(f"[warm] Ep{ep} tr {trL:.4f}/{trF:.3f} | dv {dvL:.4f}/{dvF:.3f}")

    best,pat,best_state = -1, PATIENCE, None
    for ep in range(1, EPOCHS+1):
        trL,trF,_,_,_ = run_epoch(Ltr, True)
        dvL,dvF,_,_,_ = run_epoch(Ldv, False)
        print(f"[{tag}] Ep{ep} tr {trL:.4f}/{trF:.3f} | dv {dvL:.4f}/{dvF:.3f}")
        if dvF>best: best,pat,best_state = dvF,PATIENCE,{k:v.cpu().clone() for k,v in model.state_dict().items()}
        else:
            pat -= 1
            if pat==0: break
    if best_state is not None: model.load_state_dict(best_state)

    teL,teF,te_pred,te_gold,te_prob = run_epoch(Lte, False)
    rep = classification_report(te_gold, te_pred, target_names=LABELS, digits=3, output_dict=True)
    pd.DataFrame(rep).to_csv(EXP/"reports"/"test_report.csv", encoding="utf-8-sig")
    cm  = confusion_matrix(te_gold, te_pred, labels=list(range(len(LABELS))))
    pd.DataFrame(cm, index=LABELS, columns=LABELS).to_csv(EXP/"reports"/"confusion_matrix.csv", encoding="utf-8-sig")

    # Threshold scan of Zh-Dur (0.30~0.70)
    dur_id = LABELS.index("Zh-Dur")
    y_true = (te_gold==dur_id).astype(int)
    y_sc   = te_prob[:, dur_id]
    rows=[]
    for t in np.linspace(0.30,0.70,41):
        y_hat = (y_sc>=t).astype(int)
        tp = int((y_hat & (y_true==1)).sum())
        p = tp / max(1, y_hat.sum())
        r = tp / max(1, y_true.sum())
        f = (2*p*r)/(p+r) if (p+r)>0 else 0.0
        rows.append({"tau":float(t),"precision":float(p),"recall":float(r),"f1":float(f)})
    pd.DataFrame(rows).to_csv(EXP/"reports"/"PR_Zh-Dur_thresh_scan.csv", index=False, encoding="utf-8-sig")
    print(f"[{tag}] test macro-F1={teF:.3f}; reports saved → {EXP/'reports'}")
    return EXP

# ==== Run (using df_train / df_dev / df_test obtained from scheme D) ====
_use_cols = ["epoch","layer","book_id","century","left_ctx","zhe","right_ctx","label_gold","row_id"]
df_train_use = df_train[_use_cols].copy()
df_dev_use   = df_dev[_use_cols].copy()
df_test_use  = df_test[_use_cols].copy()

# A) Gold Label Only
_ = train_eval(df_train_use, df_dev_use, df_test_use, tag="gold_scratch", warm_silver=None)

# B) Silver → Gold
try:
    df_silver_zhe
    need = ["epoch","layer","book_id","left_ctx","right_ctx","zhe","label_silver"]
    if all(c in df_silver_zhe.columns for c in need) and len(df_silver_zhe):
        _ = train_eval(df_train_use, df_dev_use, df_test_use, tag="silver2gold", warm_silver=df_silver_zhe[need])
    else:
        print("[silver2gold] 跳过：df_silver_zhe 不可用或缺列。")
except NameError:
    print("[silver2gold] 跳过：未发现 df_silver_zhe。")

# === Block 2 (no-ckpt): Directly read the evaluation results of E) → Summary
import pandas as pd, numpy as np, json, textwrap
from pathlib import Path

GOLD_DIR = Path("/content/drive/MyDrive/Papers/S1/dataset/NN training/ZHE_gold")
LABELS = ["Zh-LexV","Zh-P","Zh-Dur?","Zh-Dur"]

# Select one review version to summarize (prioritize silver2gold; use gold_scratch if it doesn't exist)
EXP_BASE = GOLD_DIR / "experiments" / "GOLD_v1"
CAND = [EXP_BASE/"silver2gold"/"reports", EXP_BASE/"gold_scratch"/"reports"]
REP_DIR = next((p for p in CAND if p.exists()), None)
if REP_DIR is None:
    raise FileNotFoundError("未找到任何 reports 目录。请先完成 E) 训练评测。")

# Read in three outputs
rep_csv = REP_DIR / "test_report.csv"
cm_csv  = REP_DIR / "confusion_matrix.csv"
pr_csv  = REP_DIR / "PR_Zh-Dur_thresh_scan.csv"

rep = pd.read_csv(rep_csv, index_col=0)         # Rows: precision/recall/f1-score/support; Columns: various types + 'macro avg' + 'weighted avg'
cm  = pd.read_csv(cm_csv, index_col=0)
pr  = pd.read_csv(pr_csv)

# Parsing macro-F1 (and various P/R/F1)
macro_f1 = float(rep.loc["f1-score", "macro avg"]) if "macro avg" in rep.columns else np.nan
per_class = (
    rep.loc[["precision","recall","f1-score"], LABELS]
      .rename_axis("metric").reset_index()
      .melt(id_vars="metric", var_name="label", value_name="value")
      .pivot(index="label", columns="metric", values="value")
      .reset_index()
)

# Write a brief summary (without changing any existing files)
summary = {
    "exp_reports_dir": str(REP_DIR),
    "macro_f1": macro_f1,
    "per_class": per_class.to_dict(orient="records"),
    "confusion_matrix_index": cm.index.tolist(),
    "confusion_matrix_columns": cm.columns.tolist(),
    "pr_points": len(pr),
    "pr_tau_min": float(pr["tau"].min()) if "tau" in pr.columns else None,
    "pr_tau_max": float(pr["tau"].max()) if "tau" in pr.columns else None,
}
(GOLD_DIR/"EVAL_SUMMARY.json").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")

# Makes eye examination easier
print(f"[OK] 汇总自：{REP_DIR}")
print(f"macro-F1 = {macro_f1:.3f}")
print("per-class（P/R/F1）：")
print(per_class.to_string(index=False))
print("\nConfusion matrix：")
print(cm)

# (Optional) Copy the three files to the GOLD_DIR root directory for quick viewing.
rep.to_csv(GOLD_DIR/"test_report_flat.csv", encoding="utf-8-sig")
cm.to_csv(GOLD_DIR/"test_confusion_matrix.csv", encoding="utf-8-sig")
pr.to_csv(GOLD_DIR/"PR_Zh-Dur_thresh_scan.csv", index=False, encoding="utf-8-sig")

# Note: This Block2 does not perform per-sample inference and does not generate test_eval_dump.parquet.
# If one does need per-sample probabilities for TM/CA later, there can be a separate Block2-proba created (we'll see),
# But that will require obtaining the model weights once (best.pt or the current in-memory model).

# === Block 3 (no-OUT_DUMP): macro/per-class metrics + confusion (read E) saved report)

# Select one of the experiment's reports (prioritize silver2gold, otherwise gold_scratch)
EXP_BASE = GOLD_DIR / "experiments" / "GOLD_v1"
CAND = [EXP_BASE/"silver2gold"/"reports", EXP_BASE/"gold_scratch"/"reports"]
REP_DIR = next((p for p in CAND if p.exists()), None)
if REP_DIR is None:
    raise FileNotFoundError("未找到任何 reports 目录，请先完成 E) 训练评测。")

# Read ready-made aggregated reports
rep_csv = REP_DIR / "test_report.csv"
cm_csv  = REP_DIR / "confusion_matrix.csv"
rep_df = pd.read_csv(rep_csv, index_col=0)
cm_df  = pd.read_csv(cm_csv,  index_col=0)

# Take macro-F1 and various indicators
macro_f1 = float(rep_df.loc["f1-score", "macro avg"]) if "macro avg" in rep_df.columns else float("nan")
per_class = (
    rep_df.loc[["precision","recall","f1-score"], LABELS]
         .rename_axis("metric").reset_index()
         .melt(id_vars="metric", var_name="label", value_name="value")
         .pivot(index="label", columns="metric", values="value")
         .reset_index()
)

# Save a copy to the root directory for quick viewing (this will not overwrite the original result).
REP_FLAT = GOLD_DIR / "test_report_flat.csv"
CONF_FLAT = GOLD_DIR / "test_confusion_matrix.csv"
per_class.to_csv(REP_FLAT, index=False, encoding="utf-8-sig")
cm_df.to_csv(CONF_FLAT, encoding="utf-8-sig")

print(f"[OK] 来自：{REP_DIR}")
print(f"macro-F1 = {macro_f1:.3f}")
print("per-class（P/R/F1）：")
print(per_class.to_string(index=False))
print("\nConfusion matrix：")
print(cm_df)

# === Block 4 (no-OUT_DUMP): PR curve for Zh-Dur + threshold sweep (read E) saved scan)

# Threshold scan file (E) already exists in the same directory (written during the training phase)
# Prioritize silver2gold, then revert to gold_scratch
EXP_BASE = GOLD_DIR / "experiments" / "GOLD_v1"
CAND = [EXP_BASE/"silver2gold"/"reports", EXP_BASE/"gold_scratch"/"reports"]
REP_DIR = next((p for p in CAND if p.exists()), None)
if REP_DIR is None:
    raise FileNotFoundError("未找到任何 reports 目录，请先完成 E) 训练评测。")

PR_SRC = REP_DIR / "PR_Zh-Dur_thresh_scan.csv"
pr_df = pd.read_csv(PR_SRC)

# Provide an operating point of "prioritizing recall, then f1"
cand = (pr_df.sort_values(["recall","f1","precision"], ascending=[False,False,False])
             .head(1).iloc[0])
rec_str = f"precision={cand['precision']:.3f}, recall={cand['recall']:.3f}, f1={cand['f1']:.3f} @ thresh={cand['tau']:.3f}"

# Save a copy (optional)
PR_COPY = GOLD_DIR / "zh_dur_pr_curve.csv"
pr_df.to_csv(PR_COPY, index=False, encoding="utf-8-sig")

print(f"[OK] PR from: {PR_SRC}")
print("Recommended operating point:", rec_str)

# === Block 5: DATA CARD & BASIC STATS ===
import re, textwrap, pandas as pd, numpy as np
from pathlib import Path

# —— Reuse or fallback Path ——
GOLD_DIR = Path("/data/NN training/ZHE_gold")
OUT_GOLD_V1  = GOLD_DIR / "zhe_gold_GOLD_v1.csv"
OUT_TRAIN    = GOLD_DIR / "zhe_gold_TRAIN_v1.csv"
OUT_DEV      = GOLD_DIR / "zhe_gold_DEV_v1.csv"
OUT_TEST_TXT = GOLD_DIR / "zhe_gold_TEST_v1.txt"
DATA_CARD_MD = GOLD_DIR / "DATA_CARD.md"
LABELS = ["Zh-LexV","Zh-P","Zh-Dur?","Zh-Dur"]

def read_csv_smart(path, encs=("utf-8-sig","utf-8","gb18030","cp936")):
    last=None
    for e in encs:
        try: return pd.read_csv(path, dtype=str, encoding=e).fillna("")
        except Exception as ex: last=ex
    raise last

df_gold  = read_csv_smart(OUT_GOLD_V1)
df_train = read_csv_smart(OUT_TRAIN)    if OUT_TRAIN.exists() else pd.DataFrame()
df_dev   = read_csv_smart(OUT_DEV)      if OUT_DEV.exists()   else pd.DataFrame()
test_ids = [int(x.strip()) for x in open(OUT_TEST_TXT, encoding="utf-8").read().splitlines()] if OUT_TEST_TXT.exists() else []
df_test  = df_gold[df_gold["row_id"].astype(int).isin(test_ids)].copy() if len(test_ids) else pd.DataFrame()

# — Trigger (consistent with previous text) —
HAN = r"[\u4e00-\u9fff]"
LOC = r"[上下内中前後东西南北於于在間里裏內外旁上中下前后左右內間之]"
NEG_SET = set(list("不未無无没沒"))
def mark_triggers(row):
    left = str(row.get("left_ctx","") or "")
    right= str(row.get("right_ctx","") or "")
    zhe  = str(row.get("zhe","") or "")
    full = left + zhe + right
    A = (re.search(fr"(?<!所){HAN}著{HAN}", full) is not None)
    B = (re.search(fr"{LOC}", full) is not None)
    C = any(ch in NEG_SET for ch in full)
    D = bool(re.search(r"著\((Di|T)\)", full))
    E = not (A or B or C or D)
    return pd.Series({"trig_A":A,"trig_B":B,"trig_C":C,"trig_D":D,"trig_E":E})

# Add a trigger for statistics (without writing back to the original file)
df_gold_trig = pd.concat([df_gold, df_gold.apply(mark_triggers, axis=1)], axis=1)

# — Statistical Table: Category Distribution & Trigger Percentage (Overall & Sub-label) —
label_dist = df_gold_trig["label_gold"].value_counts().reindex(LABELS, fill_value=0)
trig_cols = ["trig_A","trig_B","trig_C","trig_D","trig_E"]
trig_overall = df_gold_trig[trig_cols].mean().rename("ratio").to_frame()

trig_by_label = (
    df_gold_trig.groupby("label_gold")[trig_cols]
    .mean()
    .reindex(LABELS)
)

# 导出统计 CSV（不覆盖训练产物）
STATS_DIR = GOLD_DIR / "stats"
STATS_DIR.mkdir(parents=True, exist_ok=True)
(label_dist.rename("count").to_frame().to_csv(STATS_DIR/"label_distribution_gold_v1.csv", encoding="utf-8-sig"))
trig_overall.to_csv(STATS_DIR/"trigger_overall_gold_v1.csv", encoding="utf-8-sig")
trig_by_label.to_csv(STATS_DIR/"trigger_by_label_gold_v1.csv", encoding="utf-8-sig")

# —— 写 Data Card（追加/覆盖均可） ——
sizes = {
    "GOLD_v1": len(df_gold),
    "TRAIN":   len(df_train),
    "DEV":     len(df_dev),
    "TEST":    len(df_test),
}
card = textwrap.dedent(f"""
# ZHE GOLD_v1 Data Card

## Versioning
- seed: 42
- data dir: `{GOLD_DIR}`
- artifacts:
  - GOLD_v1: `{OUT_GOLD_V1.name}` (n={sizes['GOLD_v1']})
  - TRAIN: `{OUT_TRAIN.name}` (n={sizes['TRAIN']})
  - DEV: `{OUT_DEV.name}` (n={sizes['DEV']})
  - TEST ids: `{OUT_TEST_TXT.name}` (n={sizes['TEST']})

## Label distribution (GOLD_v1)
{label_dist.to_string()}

## Trigger ratios (overall)
{trig_overall['ratio'].round(3).to_string()}

## Notes
- TEST_v1 is frozen by row_id list.
- Train/Dev split is by book_id grouping to reduce leakage.
- Triggers are computed from (left_ctx, zhe, right_ctx) at report time only.
""").strip()+"\n"

DATA_CARD_MD.write_text(card, encoding="utf-8")
print("[DATA_CARD] updated →", DATA_CARD_MD)
print("[STATS] saved →", STATS_DIR)

# === Block 6: ERROR BUCKETS (Top examples per confusion pair) ===
import numpy as np, pandas as pd, torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from collections import defaultdict

GOLD_DIR = Path("/content/drive/MyDrive/Papers/S1/dataset/NN training/ZHE_gold")
LABELS = ["Zh-LexV","Zh-P","Zh-Dur?","Zh-Dur"]

OUT_DUMP = GOLD_DIR / "test_eval_dump.parquet"
OUT_TRAIN = GOLD_DIR / "zhe_gold_TRAIN_v1.csv"
OUT_DEV   = GOLD_DIR / "zhe_gold_DEV_v1.csv"
OUT_TEST  = GOLD_DIR / "zhe_gold_TEST_v1_preview.csv"  # Block C-lite outputs
TEST_TXT  = GOLD_DIR / "zhe_gold_TEST_v1.txt"          # row_id list
BUCKET_DIR= GOLD_DIR / "error_buckets"
BUCKET_DIR.mkdir(parents=True, exist_ok=True)

def read_csv_smart(path, encs=("utf-8-sig","utf-8","gb18030","cp936")):
    last=None
    for e in encs:
        try: return pd.read_csv(path, dtype=str, encoding=e).fillna("")
        except Exception as ex: last=ex
    raise last

# ——— Path A: If OUT_DUMP exists, use it directly ———
if OUT_DUMP.exists():
    dump = pd.read_parquet(OUT_DUMP)
    df_test = dump.copy()
    y_true = df_test["gold_label"].values
    y_pred = df_test[[f"p_{l}" for l in LABELS]].values.argmax(axis=1)
    y_pred = np.array([LABELS[i] for i in y_pred])
    df_test["pred_label"] = y_pred
    print("[A] using existing OUT_DUMP:", OUT_DUMP)

else:
    # ——— Path B: None OUT_DUMP → Lightweight Inference (No Training) ———
    print("[B] OUT_DUMP not found; doing a light forward pass with saved best.pt")

    # Assemble df_test (restore from GOLD_v1 using the row_id list)
    df_gold = read_csv_smart(GOLD_DIR/"zhe_gold_GOLD_v1.csv")
    if TEST_TXT.exists():
        test_ids = [int(x.strip()) for x in open(TEST_TXT, encoding="utf-8").read().splitlines()]
        df_test = df_gold[df_gold["row_id"].astype(int).isin(test_ids)].copy()
    elif OUT_TEST.exists():
        df_test = read_csv_smart(OUT_TEST)
    else:
        raise FileNotFoundError("找不到 TEST 集（既无 TEST_v1.txt 也无 preview.csv）。")

    # Building a character table using TRAIN
    df_train = read_csv_smart(OUT_TRAIN) if OUT_TRAIN.exists() else df_gold
    L_CTX, R_CTX = 40, 40
    def build_char_vocab(df):
        from collections import Counter
        cnt = Counter()
        for a,b in zip(df["left_ctx"], df["right_ctx"]):
            if isinstance(a,str): cnt.update(list(a[-L_CTX:]))
            if isinstance(b,str): cnt.update(list(b[:R_CTX]))
        cnt.update(["著"])
        itos = ["<pad>","<unk>"] + [ch for ch,_ in cnt.items()]
        stoi = {ch:i for i,ch in enumerate(itos)}
        return stoi, itos
    def encode_window(left,right,stoi):
        l = list((left or "")[-L_CTX:]); r = list((right or "")[:R_CTX])
        ids = [stoi.get(ch, stoi["<unk>"]) for ch in (l+["著"]+r)]
        MAX = L_CTX+1+R_CTX
        if len(ids)<MAX: ids += [stoi["<pad>"]] * (MAX-len(ids))
        return np.array(ids[:MAX], dtype=np.int64)

    stoi,itos = build_char_vocab(df_train)

    class TestSet(Dataset):
        def __init__(self, df, stoi):
            self.df=df.reset_index(drop=True); self.stoi=stoi
        def __len__(self): return len(self.df)
        def __getitem__(self,i):
            r=self.df.iloc[i]
            x=encode_window(r["left_ctx"], r["right_ctx"], self.stoi)
            layer=int(pd.to_numeric(r.get("layer",0), errors="coerce") or 0)
            return torch.tensor(x), torch.tensor(layer)

    # Network consistent with training
    class CharCNN_BiLSTM_Att(nn.Module):
        def __init__(self, vocab_size, nlab, emb=128, cnn_ch=128, lstm_h=128, side=8):
            super().__init__()
            self.emb = nn.Embedding(vocab_size, emb, padding_idx=0)
            self.c2, self.c3, self.c4 = nn.Conv1d(emb, cnn_ch, 2), nn.Conv1d(emb, cnn_ch, 3), nn.Conv1d(emb, cnn_ch, 4)
            self.lstm = nn.LSTM(emb, lstm_h, batch_first=True, bidirectional=True)
            self.layer_emb = nn.Embedding(6, side)
            self.fc1 = nn.Linear(cnn_ch*3 + lstm_h*2 + side, 256)
            self.fc2 = nn.Linear(256, len(LABELS))
            self.drop = nn.Dropout(0.3)
        def forward(self, x_ids, layer):
            x = self.emb(x_ids)
            xc = x.transpose(1,2)
            cmax = torch.cat([
                torch.max(torch.relu(self.c2(xc)), dim=-1)[0],
                torch.max(torch.relu(self.c3(xc)), dim=-1)[0],
                torch.max(torch.relu(self.c4(xc)), dim=-1)[0],
            ], dim=1)
            lstm_out,_ = self.lstm(x)
            att = torch.softmax((lstm_out**2).sum(-1), dim=-1).unsqueeze(1)
            lstm_vec = torch.bmm(att, lstm_out).squeeze(1)
            feat = torch.cat([cmax, lstm_vec, self.layer_emb(layer)], dim=1)
            h = torch.relu(self.fc1(feat)); h = self.drop(h)
            return self.fc2(h)

    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # 选择一个已保存的 best.pt（优先 silver2gold）
    EXP_BASE = GOLD_DIR / "experiments" / "GOLD_v1"
    CKPT = (EXP_BASE/"silver2gold"/"best.pt") if (EXP_BASE/"silver2gold"/"best.pt").exists() else (EXP_BASE/"gold_scratch"/"best.pt")
    if not CKPT.exists():
        raise FileNotFoundError("未找到 best.pt（silver2gold 或 gold_scratch）。先运行 E) 的训练块。")

    state = torch.load(CKPT, map_location="cpu")
    model = CharCNN_BiLSTM_Att(len(itos), len(LABELS)).to(DEVICE)
    model.load_state_dict(state if isinstance(state, dict) else state["state_dict"], strict=False)
    model.eval()

    ds = TestSet(df_test, stoi)
    loader = DataLoader(ds, batch_size=128, shuffle=False)
    all_prob=[]
    with torch.no_grad():
        for x,layer in loader:
            x,layer = x.to(DEVICE), layer.to(DEVICE)
            logits = model(x,layer).cpu().numpy()
            logits -= logits.max(axis=1, keepdims=True)
            prob = np.exp(logits); prob /= prob.sum(axis=1, keepdims=True)
            all_prob.append(prob)
    proba = np.concatenate(all_prob,0)
    pred_ids = proba.argmax(1)
    df_test["pred_label"] = [LABELS[i] for i in pred_ids]
    # （可选）缓存一份 dump，供以后直接用
    dump = pd.concat([df_test.reset_index(drop=True),
                      pd.DataFrame({f"p_{l}":proba[:,i] for i,l in enumerate(LABELS)})], axis=1)
    dump.to_parquet(OUT_DUMP, index=False)
    print("[B] wrote OUT_DUMP:", OUT_DUMP)

# —— 生成“误差分桶 Top 示例”——
df_err = df_test[df_test["pred_label"] != df_test["gold_label"]].copy()
pair_counts = df_err.groupby(["gold_label","pred_label"]).size().reset_index(name="n") \
                    .sort_values("n", ascending=False)

# 你最关注的对：Dur↔Dur?、LexV↔P
focus_pairs = [("Zh-Dur?","Zh-Dur"), ("Zh-Dur","Zh-Dur?"), ("Zh-LexV","Zh-P"), ("Zh-P","Zh-LexV")]
# 再加上出现次数前若干对
top_pairs = pair_counts.head(6)[["gold_label","pred_label"]].itertuples(index=False, name=None)
for p in top_pairs:
    if p not in focus_pairs: focus_pairs.append(p)

def pick_examples(df, gold, pred, k=12):
    sub = df[(df["gold_label"]==gold)&(df["pred_label"]==pred)].copy()
    # 触发器优先多样（A..E），再随机
    trig_cols = [c for c in sub.columns if c.startswith("trig_")]
    if not trig_cols:
        # 没有触发器列就直接随机
        return sub.sample(n=min(k, len(sub)), random_state=42)[["epoch","layer","book_id","century","left_ctx","zhe","right_ctx","gold_label","pred_label"]]
    parts=[]
    for t in ["trig_A","trig_B","trig_C","trig_D","trig_E"]:
        if t in trig_cols:
            ss = sub[sub[t]==True]
            if len(ss): parts.append(ss.sample(n=min(max(1, k//5), len(ss)), random_state=42))
    if parts: subx = pd.concat(parts).drop_duplicates()
    else:     subx = sub
    if len(subx) < k:
        need = k - len(subx)
        more = sub.drop(index=subx.index) if len(subx) else sub
        if len(more): subx = pd.concat([subx, more.sample(n=min(need, len(more)), random_state=42)])
    return subx.head(k)[["epoch","layer","book_id","century","left_ctx","zhe","right_ctx","gold_label","pred_label"]]

# Export the top examples for each obfuscated pair
for (g,p) in focus_pairs:
    if g==p: continue
    ex = pick_examples(df_err, g, p, k=12)
    if len(ex):
        out = BUCKET_DIR / f"errors_{g}_to_{p}.csv"
        ex.to_csv(out, index=False, encoding="utf-8-sig")
        print("saved:", out)

# Compile a list
pair_counts.to_csv(BUCKET_DIR/"error_pairs_counts.csv", index=False, encoding="utf-8-sig")
print("saved:", BUCKET_DIR/"error_pairs_counts.csv")

# ==== Ablation Runner (M0/M1) ====
import os, numpy as np, pandas as pd, torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, f1_score
from pathlib import Path

# — Reuse your global variables —
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
LABELS = ["Zh-LexV","Zh-P","Zh-Dur?","Zh-Dur"]
LAB2ID = {l:i for i,l in enumerate(LABELS)}
GOLD_DIR = Path("/content/drive/MyDrive/Papers/S1/dataset/NN training/ZHE_gold")
SEED=42; np.random.seed(SEED)

# Use the three sets already obtained in section D
_use_cols = ["epoch","layer","book_id","century","left_ctx","zhe","right_ctx","label_gold","row_id"]
df_tr = df_train[_use_cols].reset_index(drop=True).copy()
df_dv = df_dev[_use_cols].reset_index(drop=True).copy()
df_te = df_test[_use_cols].reset_index(drop=True).copy()

L_CTX, R_CTX = 40, 40
_NEG = set(list("不未無无没沒"))
_ASP = set(list("了過过着著呢"))

def trig_side3(left, right):
    s = (left or "") + (right or "")
    neg = int(any(ch in s for ch in _NEG))
    asp = int(any(ch in s for ch in _ASP))
    rn  = int((right or "")[:1] in ("呢","着","著","了"))
    return np.array([neg, asp, rn], dtype=np.float32)

def build_char_vocab(df):
    from collections import Counter
    cnt = Counter()
    for a,b in zip(df["left_ctx"], df["right_ctx"]):
        if isinstance(a,str): cnt.update(list(a[-L_CTX:]))
        if isinstance(b,str): cnt.update(list(b[:R_CTX]))
    cnt.update(["著"])
    itos = ["<pad>","<unk>"] + [ch for ch,_ in cnt.items()]
    stoi = {ch:i for i,ch in enumerate(itos)}
    return stoi, itos

def enc_window(left,right,stoi):
    l = list((left or "")[-L_CTX:])
    r = list((right or "")[:R_CTX])
    ids = [stoi.get(ch, stoi["<unk>"]) for ch in (l+["著"]+r)]
    MAX = L_CTX+1+R_CTX
    if len(ids)<MAX: ids += [stoi["<pad>"]] * (MAX-len(ids))
    return np.array(ids[:MAX], dtype=np.int64)

class GoldSet(Dataset):
    def __init__(self, df, stoi, use_layer, use_side3):
        self.df=df; self.stoi=stoi; self.use_layer=use_layer; self.use_side3=use_side3
    def __len__(self): return len(self.df)
    def __getitem__(self,i):
        r = self.df.iloc[i]
        x = enc_window(r["left_ctx"], r["right_ctx"], self.stoi)
        y = LAB2ID[r["label_gold"]]
        lay = int(pd.to_numeric(r.get("layer",0), errors="coerce") or 0) if self.use_layer else 0
        s3 = trig_side3(r["left_ctx"], r["right_ctx"]) if self.use_side3 else np.zeros(3, np.float32)
        return (torch.tensor(x), torch.tensor(lay), torch.tensor(s3, dtype=torch.float32), torch.tensor(y))

class CharCNN_BiLSTM_Att_V(nn.Module):
    def __init__(self, vocab_size, nlab, use_layer, use_side3, emb=128, cnn_ch=128, lstm_h=128, side_dim=8):
        super().__init__()
        self.use_layer, self.use_side3 = use_layer, use_side3
        self.emb = nn.Embedding(vocab_size, emb, padding_idx=0)
        self.c2=self._conv(emb,cnn_ch,2); self.c3=self._conv(emb,cnn_ch,3); self.c4=self._conv(emb,cnn_ch,4)
        self.lstm = nn.LSTM(emb, lstm_h, batch_first=True, bidirectional=True)
        self.drop = nn.Dropout(0.3)
        in_dim = cnn_ch*3 + lstm_h*2
        if self.use_layer:
            self.layer_emb = nn.Embedding(6, side_dim); in_dim += side_dim
        if self.use_side3:
            in_dim += 3
        self.fc1 = nn.Linear(in_dim, 256)
        self.fc2 = nn.Linear(256, nlab)
    def _conv(self, emb, ch, k): return nn.Conv1d(emb, ch, k)
    def forward(self, x_ids, layer, side3):
        x = self.emb(x_ids)                  # [B,T,E]
        xc = x.transpose(1,2)
        cmax = torch.cat([
            torch.max(torch.relu(self.c2(xc)), dim=-1)[0],
            torch.max(torch.relu(self.c3(xc)), dim=-1)[0],
            torch.max(torch.relu(self.c4(xc)), dim=-1)[0]], dim=1)
        lstm_out,_ = self.lstm(x)
        att = torch.softmax((lstm_out**2).sum(-1), dim=-1).unsqueeze(1)
        lstm_vec = torch.bmm(att, lstm_out).squeeze(1)
        feats = [cmax, lstm_vec]
        if self.use_layer:
            feats.append(self.layer_emb(layer))
        if self.use_side3:
            feats.append(side3)
        h = torch.relu(self.fc1(torch.cat(feats, dim=1)))
        h = self.drop(h)
        return self.fc2(h)

def run_one(tag, use_layer, use_side3, batch=64, epochs=10, lr=3e-4, wd=1e-2):
    stoi,itos = build_char_vocab(df_tr)
    def mkloader(df):
        return DataLoader(GoldSet(df, stoi, use_layer, use_side3), batch_size=batch, shuffle=('train' in tag))
    Ltr, Ldv, Lte = mkloader(df_tr), mkloader(df_dv), mkloader(df_te)
    model = CharCNN_BiLSTM_Att_V(len(itos), len(LABELS), use_layer, use_side3).to(DEVICE)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    crit = nn.CrossEntropyLoss()
    bestF, best = -1, None
    def one_epoch(loader, train=False):
        model.train(train); tot=0; allp=[]; ally=[]; allhat=[]
        for x,lay,s3,y in loader:
            x,lay,s3,y = x.to(DEVICE), lay.to(DEVICE), s3.to(DEVICE), y.to(DEVICE)
            with torch.set_grad_enabled(train):
                logits = model(x,lay,s3)
                loss = crit(logits, y)
                if train: opt.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()
            tot += loss.item()*len(x)
            allhat.append(logits.detach().argmax(1).cpu().numpy()); ally.append(y.cpu().numpy())
        hat = np.concatenate(allhat); yv = np.concatenate(ally)
        f1 = f1_score(yv, hat, average="macro")
        return tot/len(loader.dataset), f1
    pat, PATIENCE = 3, 3
    for ep in range(1, epochs+1):
        trL,trF = one_epoch(Ltr, True)
        dvL,dvF = one_epoch(Ldv, False)
        print(f"[{tag}] Ep{ep} tr {trL:.4f}/{trF:.3f} | dv {dvL:.4f}/{dvF:.3f}")
        if dvF>bestF: bestF=dvF; best={k:v.cpu().clone() for k,v in model.state_dict().items()}; pat=PATIENCE
        else:
            pat -= 1
            if pat==0: break
    if best: model.load_state_dict(best)
    # test
    model.eval(); allp=[]; ally=[];
    with torch.no_grad():
        for x,lay,s3,y in Lte:
            x,lay,s3 = x.to(DEVICE), lay.to(DEVICE), s3.to(DEVICE)
            logits = model(x,lay,s3).cpu().numpy()
            allp.append(logits.argmax(1)); ally.append(y.numpy())
    y_hat = np.concatenate(allp); y_true = np.concatenate(ally)
    rep = classification_report(y_true, y_hat, target_names=LABELS, digits=3, output_dict=True)
    out = GOLD_DIR/"experiments"/"ABLT_v1"/tag; (out).mkdir(parents=True, exist_ok=True)
    pd.DataFrame(rep).to_csv(out/"test_report.csv", encoding="utf-8-sig")
    print(f"[{tag}] TEST macro-F1={rep['macro avg']['f1-score']:.3f} → {out/'test_report.csv'}")
    return rep['macro avg']['f1-score']

# # Running the models (very fast)
m0 = run_one("M0_ctx_only", use_layer=False, use_side3=False)
m1 = run_one("M1_ctx+layer", use_layer=True,  use_side3=False)

# Summarized into a small table
summary = pd.DataFrame([
    {"model":"M0_ctx_only","macroF1":m0},
    {"model":"M1_ctx+layer","macroF1":m1},
])
summary.to_csv(GOLD_DIR/"experiments"/"ABLT_v1"/"ablation_summary.csv", index=False, encoding="utf-8-sig")
print(summary)

# ==== Silver→Gold Historical Comparison (dtype Patch Version) ====
import pandas as pd, numpy as np
from sklearn.metrics import classification_report, confusion_matrix

need = ["epoch","layer","book_id","left_ctx","right_ctx","zhe","label_silver"]
if not ('df_silver_zhe' in globals() and all(c in df_silver_zhe.columns for c in need)):
    print("[skip] df_silver_zhe 不在内存或缺列，跳过历史对比。")
else:
    gold_key_cols = ["book_id","layer","left_ctx","zhe","right_ctx"]

    # Take the silver label and rename the column.
    silver = df_silver_zhe[need].copy()
    silver.rename(columns={"label_silver":"pred_silver"}, inplace=True)

    # Retrieve the test set (you previously used df_te, so you'll continue using it here)
    gold_test = df_te.copy()

    # === Patch: Standardize Primary Key Column dtype to String (including layer) ===
    def _coerce_keys_to_str(df, keys):
        for c in keys:
            if c == "layer":
                # Digitize → Fill missing values ​​with -1 → int → then convert to str, ensuring consistency on both sides
                df[c] = pd.to_numeric(df[c], errors="coerce").fillna(-1).astype(int).astype(str)
            else:
                df[c] = df[c].astype(str).fillna("").str.strip()
        return df

    gold_test = _coerce_keys_to_str(gold_test, gold_key_cols)
    silver    = _coerce_keys_to_str(silver,    gold_key_cols)

    # Merge
    gold_test = gold_test.merge(silver, on=gold_key_cols, how="left")

    # Only retain samples that have silver labels and are within the 4-class set.
    LABELS = ["Zh-LexV","Zh-P","Zh-Dur?","Zh-Dur"]
    sub = gold_test[gold_test["pred_silver"].isin(LABELS)].copy()

    # 4×4 migration table
    ctab = pd.crosstab(sub["label_gold"], sub["pred_silver"],
                       rownames=["gold"], colnames=["silver"], dropna=False)

    out_dir = GOLD_DIR/"experiments"/"HISTORY_v1"
    out_dir.mkdir(parents=True, exist_ok=True)
    ctab.to_csv(out_dir/"silver2gold_transfer_4x4.csv", encoding="utf-8-sig")

    # Treat the silver label as "model prediction" and the gold label as the true value, and calculate P/R/F1
    lab2id = {l:i for i,l in enumerate(LABELS)}
    y_true = sub["label_gold"].map(lab2id).values
    y_pred = sub["pred_silver"].map(lab2id).values

    rep = classification_report(y_true, y_pred, target_names=LABELS,
                                digits=3, output_dict=True)
    pd.DataFrame(rep).to_csv(out_dir/"silver_as_model_report.csv", encoding="utf-8-sig")

    cm = confusion_matrix(y_true, y_pred, labels=range(len(LABELS)))
    pd.DataFrame(cm, index=LABELS, columns=LABELS)\
      .to_csv(out_dir/"silver_as_model_confusion.csv", encoding="utf-8-sig")

    print(f"[history] 匹配到银标的样本：{len(sub)}/{len(gold_test)}")
    print(f"[history] 迁移表/报告已保存 → {out_dir}")

import pandas as pd
from pathlib import Path

root = GOLD_DIR / "experiments"

rows = []

def maybe_get_macroF1(base_path: Path, name: str, use_reports_subdir: bool = True):
    """
    A structure is processed: 'macro avg' is the column name, and 'f1-score' is the row name.
    """
    if use_reports_subdir:
        csv_path = base_path / "reports" / "test_report.csv"
    else:
        csv_path = base_path / "test_report.csv"

    if not csv_path.exists():
        print(f"[warn] {name}: 找不到文件 {csv_path}")
        return

    try:
        rep = pd.read_csv(csv_path, index_col=0)
        # Here we make the following fixed structure: index has 'f1-score', columns have 'macro avg'
        macroF1 = float(rep.loc["f1-score", "macro avg"])
        rows.append({"model": name, "macroF1": macroF1})
        print(f"[ok] 读取 {name}: macroF1={macroF1:.3f}")
    except Exception as e:
        print(f"[error] 读取 {name} 时出错: {e} | 文件: {csv_path}")

# ==== gold_scratch / silver2gold ====
# Note: If the test_report.csv file in gold_scratch/silver2gold is not in the reports/ subdirectory,
# change use_reports_subdir to False
maybe_get_macroF1(root / "GOLD_v1" / "gold_scratch", "gold_scratch", use_reports_subdir=True)
maybe_get_macroF1(root / "GOLD_v1" / "silver2gold", "silver2gold", use_reports_subdir=True)

# ==== Ablation versions (in ABLT_v1/<tag>/test_report.csv) ====
for tag in ["M0_ctx_only", "M1_ctx+layer", "M2_ctx+layer+side3"]:
    maybe_get_macroF1(root / "ABLT_v1" / tag, tag, use_reports_subdir=False)

df_sum = pd.DataFrame(rows)

if df_sum.empty:
    print("[warn] 没有成功从任何 test_report.csv 里读到 macroF1，请检查上面的 [warn]/[error] 输出。")
else:
    df_sum = df_sum.sort_values("model")
    df_sum.to_csv(root / "SUMMARY_v1.csv", index=False, encoding="utf-8-sig")
    print(df_sum)

# === Global Summary ===
import os, textwrap, pandas as pd, numpy as np
from pathlib import Path
from datetime import datetime

# —— Table of Contents and Constants (Same as above/Catch-all) ——
GOLD_DIR = Path("/data/NN training/ZHE_gold")
EXP_DIR  = GOLD_DIR / "experiments"
CARD_MD  = GOLD_DIR / "EXPERIMENT_CARD.md"     # ← Distinguish the naming convention from Block 5's DATA_CARD.md.
LABELS   = ["Zh-LexV","Zh-P","Zh-Dur?","Zh-Dur"]

# —— If you already have df_sum (macro F1 summarizes a DataFrame), use it; otherwise, try scanning from the reports. ——
def read_macro_f1_from_report(csv_path: Path):
    try:
        rep = pd.read_csv(csv_path, index_col=0)
        # Both wide and long tables are compatible (sklearn's classification_report has "f1-score" in the default column)
        if "macro avg" in rep.index:
            return float(rep.loc["macro avg", "f1-score"])
        elif "f1-score" in rep.index and "macro avg" in rep.columns:
            return float(rep.loc["f1-score", "macro avg"])
    except Exception:
        pass
    return None

def safe_models_summary():
    rows = []
    # GOLD baselines
    for name in ["gold_scratch","silver2gold"]:
        p = EXP_DIR/"GOLD_v1"/name/"reports"/"test_report.csv"
        mf1 = read_macro_f1_from_report(p)
        if mf1 is not None:
            rows.append({"model":name, "macroF1": mf1, "path": str(p)})
    # Ablations
    for name in ["M0_ctx_only","M1_ctx+layer","M2_ctx+layer+side3"]:
        p = EXP_DIR/"ABLT_v1"/name/"test_report.csv"
        mf1 = read_macro_f1_from_report(p)
        if mf1 is not None:
            rows.append({"model":name, "macroF1": mf1, "path": str(p)})
    return pd.DataFrame(rows) if rows else pd.DataFrame(columns=["model","macroF1","path"])

# Prioritize reusing df_sum from the session; otherwise, scan the disk
if "df_sum" in globals() and isinstance(df_sum, pd.DataFrame) and {"model","macroF1"}.issubset(df_sum.columns):
    sum_df = df_sum.copy()
    # Attempt to complete the path column (optional)
    if "path" not in sum_df.columns:
        sum_df["path"] = ""
else:
    sum_df = safe_models_summary()

# —— Optional: Read two Zh-Dur PR curves and extract the recommended operating point (if it exists) ——
def read_op_point(exp_name: str):
    pr_csv = EXP_DIR/"GOLD_v1"/exp_name/"reports"/"PR_Zh-Dur_thresh_scan.csv"
    if not pr_csv.exists():
        return None
    try:
        df = pd.read_csv(pr_csv)
        # Strategy: Prioritize recall, then F1, and finally precision.
        pick = (df.sort_values(["recall","f1","precision"], ascending=[False,False,False])
                  .head(1).iloc[0])
        return {
            "tau": float(pick.get("tau", pick.get("thresh", np.nan))),
            "precision": float(pick["precision"]),
            "recall": float(pick["recall"]),
            "f1": float(pick["f1"]),
            "path": str(pr_csv)
        }
    except Exception:
        return None

op_gold   = read_op_point("gold_scratch")
op_s2g    = read_op_point("silver2gold")

# —— Data Product Size (Aligned with Block 5) ——
OUT_GOLD_V1  = GOLD_DIR / "zhe_gold_GOLD_v1.csv"
OUT_TRAIN    = GOLD_DIR / "zhe_gold_TRAIN_v1.csv"
OUT_DEV      = GOLD_DIR / "zhe_gold_DEV_v1.csv"
OUT_TEST_TXT = GOLD_DIR / "zhe_gold_TEST_v1.txt"

def safe_len_csv(p: Path):
    try:
        if p.exists():
            return len(pd.read_csv(p, dtype=str, encoding="utf-8-sig"))
    except Exception:
        pass
    return 0

sizes = {
    "GOLD_v1": safe_len_csv(OUT_GOLD_V1),
    "TRAIN":   safe_len_csv(OUT_TRAIN),
    "DEV":     safe_len_csv(OUT_DEV),
    "TEST":    (sum(1 for _ in open(OUT_TEST_TXT, encoding="utf-8")) if OUT_TEST_TXT.exists() else 0),
}

# —— Select the best model row ——
best_row = None
if not sum_df.empty:
    best_idx = sum_df["macroF1"].astype(float).idxmax()
    best_row = sum_df.loc[best_idx].to_dict()

# —— Assemble Markdown ——
ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
sum_tbl = (sum_df.sort_values("model")
                 .assign(macroF1=lambda d: d["macroF1"].map(lambda x: f"{x:.3f}"))
                 .to_string(index=False))

op_lines = []
if op_s2g:
    op_lines.append(f"- silver→gold: τ={op_s2g['tau']:.2f}, P={op_s2g['precision']:.3f}, R={op_s2g['recall']:.3f}, F1={op_s2g['f1']:.3f}")
if op_gold:
    op_lines.append(f"- gold_scratch: τ={op_gold['tau']:.2f}, P={op_gold['precision']:.3f}, R={op_gold['recall']:.3f}, F1={op_gold['f1']:.3f}")
op_block = "\n".join(op_lines) if op_lines else "(not available)"

card = textwrap.dedent(f"""
# ZHE Experiments Card (Global)
*generated: {ts}*

## Data Artifacts (from Block 5)
- GOLD_v1: `{OUT_GOLD_V1.name}` (n={sizes['GOLD_v1']})
- TRAIN: `{OUT_TRAIN.name}` (n={sizes['TRAIN']})
- DEV: `{OUT_DEV.name}` (n={sizes['DEV']})
- TEST ids: `{OUT_TEST_TXT.name}` (n={sizes['TEST']}; frozen row_id list)

## Model Summary (macro-F1 on frozen TEST)
{sum_tbl if len(sum_df) else "(no reports found)"}

## Best Model
{ (f"- **{best_row['model']}** with macro-F1 **{float(best_row['macroF1']):.3f}**" if best_row else "(not available)") }

## Zh-Dur Operating Points (threshold sweep, τ∈[0.30,0.70])
{op_block}

## Repro Notes
- Seed: 42; splits locked; TEST_v1 is frozen by row_id list.
- Reports read from: `{EXP_DIR}`
- This card complements `DATA_CARD.md` (dataset-level). This file summarizes **experimental** results.
""").strip()+"\n"

CARD_MD.write_text(card, encoding="utf-8")
print("[EXPERIMENT_CARD] updated →", CARD_MD)